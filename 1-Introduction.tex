\chapter{Introduction}
Since 2012 computer vision research has been increasingly dominated by approaches using deep Convolutional Neural Networks (CNNs). These CNN based techniques have allowed for achieving significantly better results in computational image and video understanding compared to previous state-of-the-art approaches. Nowadays, there are many interesting avenues of computer vision research, such as image segmentation, object detection, object recognition, image captioning, pose detection, and many more. Since the task of computational image understanding is quite complex, the CNNs used to solve these problems often require dozens, if not hundreds of millions of parameters, in order to achieve sufficiently reliable accuracies. As the number of parameters is high, the networks require large computational resources to find optimal values for all of the variables in all of the hundreds of layers of matrices. Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs) are the tools that people use to train the parameters in the networks since the training operations are highly parallelizable, allowing for the training of networks with hundreds or even thousands of layers. The problem often is that when training, one wants to increase the batch size, but at the same time, the computations must fit in the memory of the processing unit, which can be difficult to increase. Also, as the number of parameters in the model increases, the memory required for training the networks, and the time to produce outputs increases. To combat these problems, we often have to get better or more hardware, which can be expensive or compromise in the size of the model, which may lead to unreliable performance.

Embedded devices can use these CNN based algorithms to analyze their surroundings in an environment where they can't rely on external predictions from the cloud. One of the most significant issues is that achieving scene understanding requires multiple classifiers to solve various tasks. For example, a camera-based self-driving car has numerous things it is interested in resolving using the video output of its cameras. The tasks could be, for example, finding and classifying other vehicles, traffic signs, road markings, predicting paths of vehicles, pedestrians, and other actors, understanding where it can move, and finally deciding the steering angle and acceleration for the car with the information. All these tasks have to be solved using the limited computing capability onboard the actor, and the inference time for them can't be too long in order to react to everything within a reasonable time frame. Here we have many tasks, and if every task requires its independent network to produce the outputs, and we cut down on the network sizes, the classifiers might be powerful enough to produce safe outputs. On the other hand, if we don't reduce the network sizes, our inference time might be too long for a real-time system. One way to possibly get a good compromise between inference time and model complexity is to use multi-task learning and weight sharing within the model to get a single model with multiple outputs and similar or better performance to individual classifiers.

Multi-task learning proposes some solutions to these problems but makes the training process more difficult. Still, in some cases, it is something that has to be done to get a good enough inference time and accuracy on the problem at hand. Many modern computer vision models already feature an ImageNet backbone as a part of the classifier, so this is a good part of the network to consider for sharing. 
However, finding which tasks and how much can be shared is a difficult problem to solve.

\section{Scope of study}
In this thesis, we will present state-of-the-art solutions to fundamental computer vision problems, like image classification and object detection.
We will present the architectures of these models and show how the ImageNet backbones are an essential part of solving these problems.
Using the presented solutions, we will make use of various data sets to train multi-task models and evaluate the effects of training multi-task models.

\section{Structure of thesis}
Chapter 2 will present some basic terminology for describing convolutional neural networks that will be used throughout the thesis.
We won't cover the very basics of neural networks or the exact inner workings of convolutional neural networks, and refer to \citep{deep-learning} for studying those details.

Chapter 3 covers the modern image classification models, describing their architecture and how to train them.
We will also describe why the ImageNet is such a valuable data set to so many problems.
We will demonstrate the significance of the ImageNet models by defining transfer learning and describing how it is instrumental in solving small-scale problems with the ImageNet models as backbones.

Chapter 4 covers the basics of multi-task learning as it relates to computer vision.
We will define multi-task learning and see how it relates to transfer learning.
Also, the basic differences of multi-task training to a single-task setting are described, and some examples of multi-task models are shown.
Finally, the theoretical benefits and issues that may arise are talked about and demonstrated using some examples.

Chapter 5 describes what object detection is and how to evaluate object detection models.
We will also briefly discuss the basic parts of an object detector and the difference between single and two-stage object detectors.
Finally, we will cover the EfficientDet architecture in detail and explain how to train an object detector by using it.

In chapter 6, we will cover the results of our experiments on training multi-task models.
The first experiment is about training image classifiers on plant imagery.
The second experiment is about training multi-label classifiers.
The third experiment covers training object detectors with multiple datasets.
The fourth experiment combines an object detector, an image classifier, and an image segmentation model into a single classifier.
Finally, we will summarize our experiences in training multi-task models and explain what problems we encountered.

