
@article{jang_learning_2019,
	title = {Learning {What} and {Where} to {Transfer}},
	url = {http://arxiv.org/abs/1905.05901},
	abstract = {As the application of deep learning has expanded to real-world problems with insufﬁcient volume of training data, transfer learning recently has gained much attention as means of improving the performance in such small-data regime. However, when existing methods are applied between heterogeneous architectures and tasks, it becomes more important to manage their detailed conﬁgurations and often requires exhaustive tuning on them for the desired performance. To address the issue, we propose a novel transfer learning approach based on meta-learning that can automatically learn what knowledge to transfer from the source network to where in the target network. Given source and target networks, we propose an efﬁcient training scheme to learn meta-networks that decide (a) which pairs of layers between the source and target networks should be matched for knowledge transfer and (b) which features and how much knowledge from each feature should be transferred. We validate our meta-transfer approach against recent transfer learning methods on various datasets and network architectures, on which our automated scheme signiﬁcantly outperforms the prior baselines that ﬁnd “what and where to transfer” in a hand-crafted manner.},
	language = {en},
	urldate = {2019-12-09},
	journal = {arXiv:1905.05901 [cs, stat]},
	author = {Jang, Yunhun and Lee, Hankook and Hwang, Sung Ju and Shin, Jinwoo},
	month = may,
	year = {2019},
	note = {arXiv: 1905.05901},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Jang et al. - 2019 - Learning What and Where to Transfer.pdf:/home/konstaku/Zotero/storage/264ZRMT9/Jang et al. - 2019 - Learning What and Where to Transfer.pdf:application/pdf}
}

@inproceedings{oquab_learning_2014,
	address = {Columbus, OH, USA},
	title = {Learning and {Transferring} {Mid}-level {Image} {Representations} {Using} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4799-5118-5},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909618},
	doi = {10.1109/CVPR.2014.222},
	abstract = {Convolutional neural networks (CNN) have recently shown outstanding image classiﬁcation performance in the largescale visual recognition challenge (ILSVRC2012). The success of CNNs is attributed to their ability to learn rich midlevel image representations as opposed to hand-designed low-level features used in other image classiﬁcation methods. Learning CNNs, however, amounts to estimating millions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be efﬁciently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred representation leads to signiﬁcantly improved results for object and action classiﬁcation, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.},
	language = {en},
	urldate = {2020-01-29},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
	month = jun,
	year = {2014},
	pages = {1717--1724},
	file = {Oquab et al. - 2014 - Learning and Transferring Mid-level Image Represen.pdf:/home/konstaku/Zotero/storage/3WWZQLSF/Oquab et al. - 2014 - Learning and Transferring Mid-level Image Represen.pdf:application/pdf}
}