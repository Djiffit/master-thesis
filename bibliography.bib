@article{einstein,
author =       "Albert Einstein",
title =        "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
[{On} the electrodynamics of moving bodies]",
journal =      "Annalen der Physik",
volume =       "322",
number =       "10",
pages =        "891--921",
year =         "1905",
DOI =          "http://dx.doi.org/10.1002/andp.19053221004"
}

@article{guerra_weather_2018,
title = {Weather {Classification}: {A} new multi-class dataset, data augmentation approach and comprehensive evaluations of {Convolutional} {Neural} {Networks}},
shorttitle = {Weather {Classification}},
url = {http://arxiv.org/abs/1808.00588},
abstract = {Weather conditions often disrupt the proper functioning of transportation systems. Present systems either deploy an array of sensors or use an in-vehicle camera to predict weather conditions. These solutions have resulted in incremental cost and limited scope. To ensure smooth operation of all transportation services in all-weather conditions, a reliable detection system is necessary to classify weather in wild. The challenges involved in solving this problem is that weather conditions are diverse in nature and there is an absence of discriminate features among various weather conditions. The existing works to solve this problem have been scene speciﬁc and have targeted classiﬁcation of two categories of weather. In this paper, we have created a new open source dataset consisting of images depicting three classes of weather i.e rain, snow and fog called RFS Dataset. A novel algorithm has also been proposed which has used super pixel delimiting masks as a form of data augmentation, leading to reasonable results with respect to ten Convolutional Neural Network architectures.},
language = {en},
urldate = {2019-12-03},
journal = {arXiv:1808.00588 [cs]},
author = {Guerra, Jose Carlos Villarreal and Khanam, Zeba and Ehsan, Shoaib and Stolkin, Rustam and McDonald-Maier, Klaus},
month = aug,
year = {2018},
note = {arXiv: 1808.00588},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Guerra et al. - 2018 - Weather Classification A new multi-class dataset,.pdf:/Users/kokutvon/Zotero/storage/69INUZPT/Guerra et al. - 2018 - Weather Classification A new multi-class dataset,.pdf:application/pdf}
}

@article{ibrahim_weathernet:_2019,
title = {{WeatherNet}: {Recognising} {Weather} and {Visual} {Conditions} from {Street}-{Level} {Images} {Using} {Deep} {Residual} {Learning}},
volume = {8},
issn = {2220-9964},
shorttitle = {{WeatherNet}},
url = {https://www.mdpi.com/2220-9964/8/12/549},
doi = {10.3390/ijgi8120549},
abstract = {Extracting information related to weather and visual conditions at a given time and space is indispensable for scene awareness, which strongly impacts our behaviours, from simply walking in a city to riding a bike, driving a car, or autonomous driveassistance. Despite the significance of this subject, it is still not been fully addressed by the machine intelligence relying on deep learning and computer vision to detect the multi-labels of weather and visual conditions with a unified method that can be easily used for practice. What has been achieved to-date is rather sectorial models that address limited number of labels that do not cover the wide spectrum of weather and visual conditions. Nonetheless, weather and visual conditions are often addressed individually. In this paper, we introduce a novel framework to automatically extract this information from street-level images relying on deep learning and computer vision using a unified method without any pre-defined constraints in the processed images. A pipeline of four deep Convolutional Neural Network (CNN) models, so-called the WeatherNet, is trained, relying on residual learning using ResNet50 architecture, to extract various weather and visual conditions such as Dawn/dusk, day and night for time detection, and glare for lighting conditions, and clear, rainy, snowy, and foggy for weather conditions. The WeatherNet shows strong performance in extracting this information from user-defined images or video streams that can be used not limited to: autonomous vehicles and drive-assistance systems, tracking behaviours, safety-related research, or even for better understanding cities through images for policy-makers.},
language = {en},
number = {12},
urldate = {2019-12-03},
journal = {IJGI},
author = {Ibrahim, Mohamed R. and Haworth, James and Cheng, Tao},
month = nov,
year = {2019},
pages = {549},
file = {Ibrahim et al. - 2019 - WeatherNet Recognising Weather and Visual Conditi.pdf:/Users/kokutvon/Zotero/storage/V3IZJE9G/Ibrahim et al. - 2019 - WeatherNet Recognising Weather and Visual Conditi.pdf:application/pdf}
}

@article{pan_winter_nodate,
title = {Winter {Road} {Surface} {Condition} {Recognition} {Using} a {Pre}-trained {Deep} {Convolutional} {Neural} {Network}},
language = {en},
author = {Pan, Guangyuan and Fu, Liping and Yu, Ruifan and Muresan, Matthew},
pages = {13},
file = {Pan et al. - Winter Road Surface Condition Recognition Using a .pdf:/Users/kokutvon/Zotero/storage/AQWPYSCB/Pan et al. - Winter Road Surface Condition Recognition Using a .pdf:application/pdf}
}
@book{latexcompanion,
  author    = "Michel Goossens and Frank Mittelbach and Alexander Samarin",
  title     = "The \LaTeX\ Companion",
  year      = "1993",
  publisher = "Addison-Wesley",
  address   = "Reading, Massachusetts"
}

@book{knuth99,
  author    = "Donald E. Knuth",
  title     = "Digital Typography",
  year      = "1999",
  publisher = "The Center for the Study of Language and Information",
  series    = "CLSI Lecture Notes (78)"
}

@misc{knuthwebsite,
  author    = "Donald Knuth",
  title     = "Knuth: Computers and Typesetting",
  url       = "http://www-cs-faculty.stanford.edu/~knuth/abcde.html"
}

@TECHREPORT{erkio01,
  AUTHOR  = "Erkiö, H. and Mäkelä, M. and Nykänen, M. and Verkamo, I.",
  TITLE   = "Opinnäytetyän ulkoasun malli.",
  INSTITUTION = "Tietojenkäsittelyopin laitos",
  YEAR        = 2001,
  TYPE        = "Tieteellisen kirjoittamisen kurssiin
  liittyvä julkaisematon moniste",
  ADDRESS     = "Helsinki"
}



@ARTICLE{crowdenetal79,
AUTHOR  = "Crowder, H. and Dembo, R.S. and Mulvey, J.M.",
TITLE   = "On reporting computational experiments
with mathematical software",
JOURNAL = "{ACM} Transactions on Mathematical Software",
YEAR    = 1979,
VOLUME  = 5,
NUMBER  = 2,
PAGES   = "193--203"
}

@INPROCEEDINGS{dantowsley90,
AUTHOR       = "Dan, A. and Towsley, D.",
TITLE        = "An approximate analysis of the {LRU} and {FIFO}
buffer replacement schemes",
BOOKTITLE    = "Proc. {ACM} Conf.\ Measurement and
Modeling of Computer Systems",
YEAR         = 1990,
PAGES        = "143--152",
ADDRESS      = "Boulder, Colorado, {USA}",
MONTH        = may
}

@TECHREPORT{erkio94,
  AUTHOR      = "Erkiö, H.",
  TITLE   = "Opinnäytetyän ulkoasun malli.",
  INSTITUTION = "Tietojenkäsittelyopin laitos",
  YEAR        = 1994,
  TYPE        = "Tieteellisen kirjoittamisen kurssiin
  liittyvä julkaisematon moniste",
  ADDRESS     = "Helsinki"
}

@TECHREPORT{erkiomakela96,
  AUTHOR      = "Erkiö, H. and Mäkelä, M.",
  TITLE       = "Opinnäytetyän ulkoasun malli",
  INSTITUTION = "Tietojenkäsittelytieteen laitos",
  YEAR        = 1996,
  TYPE        = "Tieteellisen kirjoittamisen kurssiin
  liittyvä julkaisematon moniste",
  ADDRESS     = "Helsinki"
}

@BOOK{fogelbergetal89,
  AUTHOR    = "Fogelberg, P. and Herranen, M. and Sinikara, K.",
  TITLE     = "Tuumasta toimeen, tutkielman tekijän opas",
  PUBLISHER = "Yliopistopaino",
  YEAR      = 1989,
  ADDRESS   = "Helsinki"
}

@INCOLLECTION{gannonetal89,
  AUTHOR    = "Gannon, D. and others",
  TITLE     = "Programming environments for parallel algorithms",
  BOOKTITLE = "Parallel and Distributed Algorithms",
  YEAR      = 1989,
  EDITOR    = "M. Cosnard and others",
  publisher    = "North-Holland",
  PAGES     = "101--108"
}

@BOOK{grimm87,
  AUTHOR    = "Grimm, S. S.",
  TITLE     = "How to write computer documentation for users",
  PUBLISHER = "Van Nostrand Reinhold Co.",
  YEAR      = 1987,
  ADDRESS   = "New York"
}

@BOOK{harkinsplung82,
  EDITOR    = "Harkins, C. and Plung, D. L.",
  TITLE     = "A guide for writing better technical papers",
  PUBLISHER = "IEEE Press",
  YEAR      = 1982}

@TECHREPORT{julkohj81,
AUTHOR      = "{Helsingin yliopisto}",
TITLE       = "Julkaisusarjoja ja opinnäytteiden tiivistelmiä
koskevat ohjeet ja suositukset",
YEAR        = 1981,
TYPE        = "Helsingin yliopiston kirjastolaitoksen julkaisu {A 3}",
ADDRESS     = "Helsinki"
}

@WWW-MISC{kilpelainen00,
AUTHOR       = "Kilpeläinen, P.",
TITLE        = "{WWW}-lähteisiin viittaaminen tutkielmatekstissä",
URL          = "http://www.cs.helsinki.fi/u/kilpelai/TiKi/wwwlahteet.html",
YEAR         =  2000,
TYPE        = "Tieteellisen kirjoittamisen kurssiin
liittyvä julkaisematon moniste",
VALID        =  "19.1.2000"
}

@ARTICLE{smith78a,
AUTHOR  = "Smith, A. J.",
TITLE   = "Sequentiality and prefetching in database systems",
JOURNAL = "{ACM} Transactions on Database Systems",
YEAR    = 1978,
VOLUME  = 3,
NUMBER  = 3,
PAGES   = "223--247"
}

@ARTICLE{smith78b,
  AUTHOR  = "Smith, A. J.",
  TITLE   = "Sequential program prefetching in memory hierarchies",
  JOURNAL = "Computer",
  YEAR    = 1978,
  VOLUME  = 11,
  NUMBER  = 11,
  PAGES   = "7--21"
}

@TECHREPORT{verkamo92,
  AUTHOR      = "Verkamo, A. I.",
  TITLE       = "Opinnäytetyän ulkoasun malli",
  INSTITUTION = "Tietojenkäsittelyopin laitos",
  YEAR        = 1992,
  TYPE        = "Tieteellisen kirjoittamisen kurssiin
  liittyvä julkaisematon moniste",
  ADDRESS     = "Helsinki"
}

@ARTICLE{abiteboul,

AUTHOR  = "Abiteboul, S. and Quass, D. and McHugh, J. and
Widom, J. and Wiener, J.L.",
TITLE   = "The Lorel query language for semistructured data",
JOURNAL = "International Journal on Digital Libraries",
YEAR    = 1997,
VOLUME  = 1,
NUMBER  = 1,
PAGES   = "68-88",
NOTE    = "[
\url{http://link.springer.de/
  link/service/journals/00799/
  bibs/7001001/70010068.htm},
18.1.2000]"
}


@misc{bray,
AUTHOR = "Bray, T. and Paoli, J. and Sperberg-McQueen, C.M.",
TITLE  = "{Extensible Markup Language {XML} 1.0. W3C Recommendation 10-February-1998}",
howpublished    = "[\url{http://www.w3.org/TR/1998/REC-xml-19980210}, 02.11.2016]"
}

@ARTICLE{dietinger,

AUTHOR  = "Dietinger, T. and others",
TITLE   = "{Dynamic Background Libraries - New Developments in
Distance Education Using {HIKS} {Hierarchical Interactive
    Knowledge System}}",
JOURNAL = "Journal of Universal Computer Science",
YEAR    = 1999,
VOLUME  = 5,
NUMBER  = 1,
NOTE    = "[\url{http://www.iicm.edu/
  jucs_5_1/dynamic_background_libraries_new},
18.1.2000]"
}

@article{alexNet,
  title = {{ImageNet} classification with deep convolutional neural networks},
  volume = {60},
  issn = {00010782},
  url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  language = {en},
  number = {6},
  urldate = {2019-12-06},
  journal = {Communications of the ACM},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  month = may,
  year = {2017},
  pages = {84--90},
  file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/home/konstaku/Zotero/storage/45HSR2RH/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf}
}

@article{imagenet,
title = {{ImageNet}: {A} {Large}-{Scale} {Hierarchical} {Image} {Database}},
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
language = {en},
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
pages = {8},
year = {2010},
file = {Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf:/home/konstaku/Zotero/storage/F5WWKAJK/Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf:application/pdf}
}


@misc{ILSVRC,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Competition} ({ILSVRC})},
	url = {http://www.image-net.org/challenges/LSVRC/},
	urldate = {2020-01-26},
	file = {ImageNet Large Scale Visual Recognition Competition (ILSVRC):/home/konstaku/Zotero/storage/2IGV2WF3/LSVRC.html:text/html}
}

@article{resNet,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2019-12-06},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/home/konstaku/Zotero/storage/CMC3Y7YX/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@article{imageNet_summary,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	url = {http://arxiv.org/abs/1409.0575},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classiﬁcation and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than ﬁfty institutions.},
	language = {en},
	urldate = {2020-01-26},
	journal = {arXiv:1409.0575 [cs]},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = jan,
	year = {2015},
	note = {arXiv: 1409.0575},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.8, I.5.2},
	file = {Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:/home/konstaku/Zotero/storage/7U9W6FSF/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:application/pdf}
}

@article{objectNet,
	title = {{ObjectNet}: {A} large-scale bias-controlled dataset for pushing the limits of object recognition models},
	abstract = {We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientiﬁc experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be ﬁne-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45\% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to ﬁne-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet – objects are largely centered and unoccluded – and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.},
	language = {en},
  year = {2019},
	author = {Barbu, Andrei and Mayo, David and Alverio, Julian and Luo, William and Wang, Christopher and Gutfreund, Dan and Tenenbaum, Josh and Katz, Boris},
	pages = {11},
	file = {Barbu et al. - ObjectNet A large-scale bias-controlled dataset f.pdf:/home/konstaku/Zotero/storage/QIHM7LV9/Barbu et al. - ObjectNet A large-scale bias-controlled dataset f.pdf:application/pdf}
}
@article{efficientNet,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a ﬁxed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefﬁcient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.},
	language = {en},
	urldate = {2019-12-09},
	journal = {arXiv:1905.11946 [cs, stat]},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = nov,
	year = {2019},
	note = {arXiv: 1905.11946},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf:/home/konstaku/Zotero/storage/IA4SQF8Z/Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf}
}

@article{betterTransfer,
	title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
	url = {http://arxiv.org/abs/1805.08974},
	abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classiﬁcation networks on 12 image classiﬁcation datasets. We ﬁnd that, when networks are used as ﬁxed feature extractors or ﬁne-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we ﬁnd that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we ﬁnd that, on two small ﬁne-grained image classiﬁcation datasets, pretraining on ImageNet provides minimal beneﬁts, indicating the learned features from ImageNet do not transfer well to ﬁne-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
	language = {en},
	urldate = {2020-01-27},
	journal = {arXiv:1805.08974 [cs, stat]},
	author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
	month = jun,
	year = {2019},
	note = {arXiv: 1805.08974},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Kornblith et al. - 2020 - Do Better ImageNet Models Transfer Better.pdf:/home/konstaku/Zotero/storage/XYMF3C3M/Kornblith et al. - 2019 - Do Better ImageNet Models Transfer Better.pdf:application/pdf}
}

@article{classifierPerformance,
	title = {Compounding the {Performance} {Improvements} of {Assembled} {Techniques} in a {Convolutional} {Neural} {Network}},
	url = {http://arxiv.org/abs/2001.06268},
	abstract = {Recent studies in image classiﬁcation have demonstrated a variety of techniques for improving the performance of Convolutional Neural Networks (CNNs). However, attempts to combine existing techniques to create a practical model are still uncommon. In this study, we carry out extensive experiments to validate that carefully assembling these techniques and applying them to a basic CNN model in combination can improve the accuracy and robustness of the model while minimizing the loss of throughput. For example, our proposed ResNet-50 shows an improvement in top-1 accuracy from 76.3\% to 82.78\%, and an mCE improvement from 76.0\% to 48.9\%, on the ImageNet ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. The resulting model signiﬁcantly outperforms state-of-theart models with similar accuracy in terms of mCE and inference throughput. To verify the performance improvement in transfer learning, ﬁne grained classiﬁcation and image retrieval tasks were tested on several open datasets and showed that the improvement to backbone network performance boosted transfer learning performance signiﬁcantly. Our approach achieved 1st place in the iFood Competition Fine-Grained Visual Recognition at CVPR 2019 1, and the source code and trained models will be made publicly available 2.},
	language = {en},
	urldate = {2020-01-27},
	journal = {arXiv:2001.06268 [cs]},
	author = {Lee, Jungkyu and Won, Taeryun and Hong, Kiho},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.06268},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Lee et al. - 2020 - Compounding the Performance Improvements of Assemb.pdf:/home/konstaku/Zotero/storage/788PI52C/Lee et al. - 2020 - Compounding the Performance Improvements of Assemb.pdf:application/pdf}
}