
@article{alexNet,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {00010782},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2017},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/home/konstaku/Zotero/storage/45HSR2RH/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf}
}

@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}



@misc{ILSVRC,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Competition} ({ILSVRC})},
	url = {http://www.image-net.org/challenges/LSVRC/},
	urldate = {2020-01-26},
	file = {ImageNet Large Scale Visual Recognition Competition (ILSVRC):/home/konstaku/Zotero/storage/2IGV2WF3/LSVRC.html:text/html}
}

@inproceedings{resNet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@article{imageNet_summary,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}


@inproceedings{objectNet,
  title={ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models},
  author={Barbu, Andrei and Mayo, David and Alverio, Julian and Luo, William and Wang, Christopher and Gutfreund, Dan and Tenenbaum, Josh and Katz, Boris},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9448--9458},
  year={2019}
}


@INPROCEEDINGS{kitti,
  author = {Jannik Fritsch and Tobias Kuehnl and Andreas Geiger},
  title = {A New Performance Measure and Evaluation Benchmark for Road Detection Algorithms},
  booktitle = {International Conference on Intelligent Transportation Systems (ITSC)},
  year = {2013}
} 







@InProceedings{efficientNet,
  title = 	 {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  author = 	 {Tan, Mingxing and Le, Quoc},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6105--6114},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  publisher = 	 {PMLR}
}




@inproceedings{betterTransfer,
  title={Do better imagenet models transfer better?},
  author={Kornblith, Simon and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2661--2671},
  year={2019}
}

@article{classifierPerformance,
title = {Compounding the {Performance} {Improvements} of {Assembled} {Techniques} in a {Convolutional} {Neural} {Network}},
url = {http://arxiv.org/abs/2001.06268},
abstract = {Recent studies in image classiﬁcation have demonstrated a variety of techniques for improving the performance of Convolutional Neural Networks (CNNs). However, attempts to combine existing techniques to create a practical model are still uncommon. In this study, we carry out extensive experiments to validate that carefully assembling these techniques and applying them to a basic CNN model in combination can improve the accuracy and robustness of the model while minimizing the loss of throughput. For example, our proposed ResNet-50 shows an improvement in top-1 accuracy from 76.3\% to 82.78\%, and an mCE improvement from 76.0\% to 48.9\%, on the ImageNet ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. The resulting model signiﬁcantly outperforms state-of-theart models with similar accuracy in terms of mCE and inference throughput. To verify the performance improvement in transfer learning, ﬁne grained classiﬁcation and image retrieval tasks were tested on several open datasets and showed that the improvement to backbone network performance boosted transfer learning performance signiﬁcantly. Our approach achieved 1st place in the iFood Competition Fine-Grained Visual Recognition at CVPR 2019 1, and the source code and trained models will be made publicly available 2.},
language = {en},
journal = {arXiv:2001.06268 [cs]},
author = {Lee, Jungkyu and Won, Taeryun and Hong, Kiho},
year = {2020},
note = {arXiv: 2001.06268},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Lee et al. - 2020 - Compounding the Performance Improvements of Assemb.pdf:/home/konstaku/Zotero/storage/788PI52C/Lee et al. - 2020 - Compounding the Performance Improvements of Assemb.pdf:application/pdf}
}

@article{leNet,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}


@inproceedings{rethinkTransfer,
  title={Rethinking imagenet pre-training},
  author={He, Kaiming and Girshick, Ross and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={4918--4927},
  year={2019}
}

@inproceedings{transferSurvey,
title = {A {Survey} of {Transfer} {Learning} for {Convolutional} {Neural} {Networks}},
doi = {10.1109/SIBGRAPI-T.2019.00010},
abstract = {Transfer learning is an emerging topic that may drive the success of machine learning in research and industry. The lack of data on specific tasks is one of the main reasons to use it, since collecting and labeling data can be very expensive and can take time, and recent concerns with privacy make difficult to use real data from users. The use of transfer learning helps to fast prototype new machine learning models using pre-trained models from a source task since training on millions of images can take time and requires expensive GPUs. In this survey, we review the concepts and definitions related to transfer learning and we list the different terms used in the literature. We bring the point of view from different authors of prior surveys, adding some more recent findings in order to give a clear vision of directions for future work in this field of research.},
booktitle = {2019 32nd {SIBGRAPI} {Conference} on {Graphics}, {Patterns} and {Images} {Tutorials} ({SIBGRAPI}-{T})},
author = {Ribani, Ricardo and Marengoni, Mauricio},
month = oct,
year = {2019},
note = {ISSN: 2474-0691},
keywords = {Machine learning, Convolutional Neural Networks, Data models, Deep Learning, Probability distribution, Supervised learning, Task analysis, Training, Transfer Learning},
pages = {47--57},
file = {IEEE Xplore Abstract Record:/home/konstaku/Zotero/storage/ZCESIJS9/8920338.html:text/html;IEEE Xplore Full Text PDF:/home/konstaku/Zotero/storage/PZN27DVW/Ribani and Marengoni - 2019 - A Survey of Transfer Learning for Convolutional Ne.pdf:application/pdf}
}

@article{transferSurvey2010,
	title = {A {Survey} on {Transfer} {Learning}},
	volume = {22},
	issn = {1041-4347},
	doi = {10.1109/TKDE.2009.191},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classiﬁcation task in one domain of interest, but we only have sufﬁcient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classiﬁcation, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
	language = {en},
	number = {10},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	year = {2010},
	pages = {1345--1359},
	file = {Pan and Yang - 2010 - A Survey on Transfer Learning.pdf:/home/konstaku/Zotero/storage/YZBBSKDB/Pan and Yang - 2010 - A Survey on Transfer Learning.pdf:application/pdf}
}

@inproceedings{denseNet,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@inproceedings{resNext,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}

@INPROCEEDINGS{mobileNetv2,

  author={M. {Sandler} and A. {Howard} and M. {Zhu} and A. {Zhmoginov} and L. {Chen}},

  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 

  title={MobileNetV2: Inverted Residuals and Linear Bottlenecks}, 

  year={2018},

  volume={},

  number={},

  pages={4510-4520},}

@incollection{gPipe,
title = {GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and Chen, zhifeng},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {103--112},
year = {2019},
publisher = {Curran Associates, Inc.},
}


@article{mobileNet,
title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
shorttitle = {{MobileNets}},
url = {http://arxiv.org/abs/1704.04861},
abstract = {We present a class of efﬁcient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyperparameters that efﬁciently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classiﬁcation. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, ﬁnegrain classiﬁcation, face attributes and large scale geo-localization.},
language = {en},
journal = {arXiv:1704.04861 [cs]},
author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
year = {2017},
note = {arXiv: 1704.04861},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:/home/konstaku/Zotero/storage/DGE2QG6B/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf}
}

        @inproceedings{wideResNet,
        	title={Wide Residual Networks},
        	author={Sergey Zagoruyko and Nikos Komodakis},
        	year={2016},
        	pages={87.1-87.12},
        	articleno={87},
        	numpages={12},
        	booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
        	publisher={BMVA Press},
        	editor={Richard C. Wilson, Edwin R. Hancock and William A. P. Smith},
        	doi={10.5244/C.30.87},
        	isbn={1-901725-59-6},
        }
@INPROCEEDINGS{mnas,

  author={M. {Tan} and B. {Chen} and R. {Pang} and V. {Vasudevan} and M. {Sandler} and A. {Howard} and Q. V. {Le}},

  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={MnasNet: Platform-Aware Neural Architecture Search for Mobile}, 

  year={2019},

  volume={},

  number={},

  pages={2815-2823},}

@incollection{neuralSearch,
	location = {Cham},
	title = {Neural Architecture Search},
	isbn = {978-3-030-05318-5},
	series = {The Springer Series on Challenges in Machine Learning},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	pages = {63--77},
	booktitle = {Automated Machine Learning: Methods, Systems, Challenges},
	publisher = {Springer International Publishing},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-030-05318-5_3},
	file = {Springer Full Text PDF:/Users/kokutvon/Zotero/storage/JXSPZ6W9/Elsken et al. - 2019 - Neural Architecture Search.pdf:application/pdf}
}



@incollection{mutualLearning,
address = {Cham},
title = {Mutual {Learning} to {Adapt} for {Joint} {Human} {Parsing} and {Pose} {Estimation}},
volume = {11209},
isbn = {978-3-030-01227-4 978-3-030-01228-1},
abstract = {This paper presents a novel Mutual Learning to Adapt model (MuLA) for joint human parsing and pose estimation. It eﬀectively exploits mutual beneﬁts from both tasks and simultaneously boosts their performance. Diﬀerent from existing post-processing or multi-task learning based methods, MuLA predicts dynamic task-speciﬁc model parameters via recurrently leveraging guidance information from its parallel tasks. Thus MuLA can fast adapt parsing and pose models to provide more powerful representations by incorporating information from their counterparts, giving more robust and accurate results. MuLA is implemented with convolutional neural networks and end-to-end trainable. Comprehensive experiments on benchmarks LIP and extended PASCALPerson-Part demonstrate the eﬀectiveness of the proposed MuLA model with superior performance to well established baselines.},
language = {en},
urldate = {2020-02-26},
booktitle = {Computer {Vision} – {ECCV} 2018},
publisher = {Springer International Publishing},
author = {Nie, Xuecheng and Feng, Jiashi and Yan, Shuicheng},
editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
year = {2018},
doi = {10.1007/978-3-030-01228-1_31},
pages = {519--534},
file = {Nie et al. - 2018 - Mutual Learning to Adapt for Joint Human Parsing a.pdf:/home/konstaku/Zotero/storage/8QYIV5FG/Nie et al. - 2018 - Mutual Learning to Adapt for Joint Human Parsing a.pdf:application/pdf}
}



@inproceedings{multiLearningUncertainty,
address = {Salt Lake City, UT, USA},
title = {Multi-task {Learning} {Using} {Uncertainty} to {Weigh} {Losses} for {Scene} {Geometry} and {Semantics}},
isbn = {978-1-5386-6420-9},
doi = {10.1109/CVPR.2018.00781},
abstract = {Numerous deep learning applications beneﬁt from multitask learning with multiple regression and classiﬁcation objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task’s loss. Tuning these weights by hand is a difﬁcult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classiﬁcation and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.},
language = {en},
urldate = {2020-01-15},
booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
publisher = {IEEE},
author = {Cipolla, Roberto and Gal, Yarin and Kendall, Alex},
year = {2018},
pages = {7482--7491},
file = {Cipolla et al. - 2018 - Multi-task Learning Using Uncertainty to Weigh Los.pdf:/home/konstaku/Zotero/storage/JUFEPU8D/Cipolla et al. - 2018 - Multi-task Learning Using Uncertainty to Weigh Los.pdf:application/pdf}
}

@inproceedings{visualPerson,
  title = {Visual Person Understanding Through Multi-task and Multi-dataset Learning},
  author = {Kilian Pfeiffer and Alexander Hermans and Istv\'{a}n S\'{a}r\'{a}ndi and Mark Weber and Bastian Leibe},
  booktitle = {German Conference on Pattern Recognition},
  pages = {551--566},
  date = {2019},
}

@article{multiDepth,
title = {{MultiDepth}: {Single}-{Image} {Depth} {Estimation} via {Multi}-{Task} {Regression} and {Classification}},
shorttitle = {{MultiDepth}},
url = {http://arxiv.org/abs/1907.11111},
abstract = {We introduce MultiDepth, a novel training strategy and convolutional neural network (CNN) architecture that allows approaching single-image depth estimation (SIDE) as a multi-task problem. SIDE is an important part of road scene understanding. It, thus, plays a vital role in advanced driver assistance systems and autonomous vehicles. Best results for the SIDE task so far have been achieved using deep CNNs. However, optimization of regression problems, such as estimating depth, is still a challenging task. For the related tasks of image classification and semantic segmentation, numerous CNN-based methods with robust training behavior have been proposed. Hence, in order to overcome the notorious instability and slow convergence of depth value regression during training, MultiDepth makes use of depth interval classification as an auxiliary task. The auxiliary task can be disabled at test-time to predict continuous depth values using the main regression branch more efficiently. We applied MultiDepth to road scenes and present results on the KITTI depth prediction dataset. In experiments, we were able to show that end-to-end multi-task learning with both, regression and classification, is able to considerably improve training and yield more accurate results.},
language = {en},
urldate = {2020-01-29},
journal = {arXiv:1907.11111 [cs]},
author = {Liebel, Lukas and Körner, Marco},
month = jul,
year = {2019},
note = {arXiv: 1907.11111},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Liebel and Körner - 2019 - MultiDepth Single-Image Depth Estimation via Mult.pdf:/home/konstaku/Zotero/storage/PWEGEBIU/Liebel and Körner - 2019 - MultiDepth Single-Image Depth Estimation via Mult.pdf:application/pdf}
}

@inproceedings{taskonomy,
  title={Taskonomy: Disentangling task transfer learning},
  author={Zamir, Amir R and Sax, Alexander and Shen, William and Guibas, Leonidas J and Malik, Jitendra and Savarese, Silvio},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3712--3722},
  year={2018}
}

@inproceedings{multiTaskAttention,
address = {Long Beach, CA, USA},
title = {End-{To}-{End} {Multi}-{Task} {Learning} {With} {Attention}},
isbn = {978-1-72813-293-8},
doi = {10.1109/CVPR.2019.00197},
abstract = {We propose a novel multi-task learning architecture, which allows learning of task-speciﬁc feature-level attention. Our design, the Multi-Task Attention Network (MTAN), consists of a single shared network containing a global feature pool, together with a soft-attention module for each task. These modules allow for learning of taskspeciﬁc features from the global features, whilst simultaneously allowing for features to be shared across different tasks. The architecture can be trained end-to-end and can be built upon any feed-forward neural network, is simple to implement, and is parameter efﬁcient. We evaluate our approach on a variety of datasets, across both image-toimage predictions and image classiﬁcation tasks. We show that our architecture is state-of-the-art in multi-task learning compared to existing methods, and is also less sensitive to various weighting schemes in the multi-task loss function. Code is available at https://github.com/ lorenmt/mtan.},
language = {en},
booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
publisher = {IEEE},
author = {Liu, Shikun and Johns, Edward and Davison, Andrew J.},
year = {2019},
pages = {1871--1880},
file = {Liu et al. - 2019 - End-To-End Multi-Task Learning With Attention.pdf:/home/konstaku/Zotero/storage/6Z9IWAEJ/Liu et al. - 2019 - End-To-End Multi-Task Learning With Attention.pdf:application/pdf}
}
@inproceedings{crossStitch,

  author={I. {Misra} and A. {Shrivastava} and A. {Gupta} and M. {Hebert}},

  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Cross-Stitch Networks for Multi-task Learning}, 

  year={2016},

  volume={},

  number={},

  pages={3994-4003},}


@article{learningToMultitask,
	title = {Learning to {Multitask}},
	abstract = {Multitask learning has shown promising performance in many applications and many multitask models have been proposed. In order to identify an effective multitask model for a given multitask problem, we propose a learning framework called Learning to MultiTask (L2MT). To achieve the goal, L2MT exploits historical multitask experience which is organized as a training set consisting of several tuples, each of which contains a multitask problem with multiple tasks, a multitask model, and the relative test error. Based on such training set, L2MT ﬁrst uses a proposed layerwise graph neural network to learn task embeddings for all the tasks in a multitask problem and then learns an estimation function to estimate the relative test error based on task embeddings and the representation of the multitask model based on a uniﬁed formulation. Given a new multitask problem, the estimation function is used to identify a suitable multitask model. Experiments on benchmark datasets show the effectiveness of the proposed L2MT framework.},
	language = {en},
	author = {Zhang, Yu and Wei, Ying and Yang, Qiang},
	pages = {12},
	file = {Zhang et al. - Learning to Multitask.pdf:/home/konstaku/Zotero/storage/CSZ75VG5/Zhang et al. - Learning to Multitask.pdf:application/pdf}
}

@inproceedings{usingUncertaintyToWeighLosses,
address = {Salt Lake City, UT, USA},
title = {Multi-task {Learning} {Using} {Uncertainty} to {Weigh} {Losses} for {Scene} {Geometry} and {Semantics}},
isbn = {978-1-5386-6420-9},
doi = {10.1109/CVPR.2018.00781},
abstract = {Numerous deep learning applications beneﬁt from multitask learning with multiple regression and classiﬁcation objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task’s loss. Tuning these weights by hand is a difﬁcult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classiﬁcation and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.},
language = {en},
booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
publisher = {IEEE},
author = {Cipolla, Roberto and Gal, Yarin and Kendall, Alex},
month = jun,
year = {2018},
pages = {7482--7491},
file = {Cipolla et al. - 2018 - Multi-task Learning Using Uncertainty to Weigh Los.pdf:/home/konstaku/Zotero/storage/JUFEPU8D/Cipolla et al. - 2018 - Multi-task Learning Using Uncertainty to Weigh Los.pdf:application/pdf}
}

@article{lossWeighting,
	title = {A {Comparison} of {Loss} {Weighting} {Strategies} for {Multi} task {Learning} in {Deep} {Neural} {Networks}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2943604},
	abstract = {With the success of deep learning in a wide variety of areas, many deep multi-task learning (MTL) models have been proposed claiming improvements in performance obtained by sharing the learned structure across several related tasks. However, the dynamics of multi-task learning in deep neural networks is still not well understood at either the theoretical or experimental level. In particular, the usefulness of different task pairs is not known a priori. Practically, this means that properly combining the losses of different tasks becomes a critical issue in multi-task learning, as different methods may yield different results. In this paper, we benchmarked different multi-task learning approaches using shared trunk with task specific branches architecture across three different MTL datasets. For the first dataset, i.e. Multi-MNIST (Modified National Institute of Standards and Technology database), we thoroughly tested several weighting strategies, including simply adding task-specific cost functions together, dynamic weight average (DWA) and uncertainty weighting methods each with various amounts of training data per-task. We find that multi-task learning typically does not improve performance for a user-defined combination of tasks. Further experiments evaluated on diverse tasks and network architectures on various datasets suggested that multi-task learning requires careful selection of both task pairs and weighting strategies to equal or exceed the performance of single task learning.},
	journal = {IEEE Access},
	author = {Gong, Ting and Lee, Tyler and Stephenson, Cory and Renduchintala, Venkata and Padhy, Suchismita and Ndirango, Anthony and Keskin, Gokce and Elibol, Oguz H.},
	year = {2019},
	keywords = {Data models, Task analysis, Training, Deep learning, deep multitask learning models, deep neural networks, dynamic weight average, Dynamic weighting average, Estimation, learning (artificial intelligence), loss weighting strategies, multi-MNIST, multi-objective optimization, multi-task learning, neural nets, task pairs, task specific branches architecture, task-specific cost functions, Training data, Uncertainty, uncertainty weighting},
	pages = {141627--141632},
	file = {IEEE Xplore Abstract Record:/home/konstaku/Zotero/storage/XGGZ8FTG/8848395.html:text/html;IEEE Xplore Full Text PDF:/home/konstaku/Zotero/storage/R9J46AED/Gong et al. - 2019 - A Comparison of Loss Weighting Strategies for Mult.pdf:application/pdf}
}

@article{surveyOnMultiTask,
title = {A {Survey} on {Multi}-{Task} {Learning}},
url = {http://arxiv.org/abs/1707.08114},
abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL. First, we classify different MTL algorithms into several categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach, and decomposition approach, and then discuss the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, batch MTL models are difﬁcult to handle this situation and online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing are reviewed to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works.},
language = {en},
journal = {arXiv:1707.08114 [cs]},
author = {Zhang, Yu and Yang, Qiang},
year = {2018},
note = {arXiv: 1707.08114},
keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
file = {Zhang and Yang - 2018 - A Survey on Multi-Task Learning.pdf:/home/konstaku/Zotero/storage/ZZDVVBMV/Zhang and Yang - 2018 - A Survey on Multi-Task Learning.pdf:application/pdf}
}

@article{origMultitask,
	title = {Multitask {Learning}},
	abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
	language = {en},
	author = {Caruana, Rich},
	year= {1997},
	pages = {35},
	file = {Caruana - Multitask Learning.pdf:/home/konstaku/Zotero/storage/5IZMB2MK/Caruana - Multitask Learning.pdf:application/pdf}
}


@article{whichTasks,
title = {Which {Tasks} {Should} {Be} {Learned} {Together} in {Multi}-task {Learning}?},
url = {http://arxiv.org/abs/1905.07553},
abstract = {Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using ‘multi-task learning’. This saves computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We systematically study task cooperation and competition and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.},
language = {en},
journal = {arXiv:1905.07553 [cs]},
author = {Standley, Trevor and Zamir, Amir R. and Chen, Dawn and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
month = may,
year = {2019},
note = {arXiv: 1905.07553},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Standley et al. - 2019 - Which Tasks Should Be Learned Together in Multi-ta.pdf:/home/konstaku/Zotero/storage/8CCVP79Y/Standley et al. - 2019 - Which Tasks Should Be Learned Together in Multi-ta.pdf:application/pdf}
}


@inproceedings{selfDriving,
address = {Budapest, Hungary},
title = {Hierarchical {Multi}-{Task} {Learning} for {Healthy} {Drink} {Classification}},
isbn = {978-1-72811-985-4},
url = {https://ieeexplore.ieee.org/document/8851796/},
doi = {10.1109/IJCNN.2019.8851796},
abstract = {Recent advances in deep convolutional neural networks have enabled convenient diet tracking exploiting photos captured with smartphone cameras. However, most of the current diet tracking apps focus on recognizing solid foods while omitting drinks despite their negative impacts on our health when consumed without moderation. After an extensive analysis of drink images, we found that such an absence is due to the following challenges that conventional convolutional neural networks trained under the single-task learning framework cannot easily handle. First, drinks are amorphous. Second, visual cues of the drinks are often occluded and distorted by their container properties. Third, ingredients are inconspicuous because they often blend into the drink. In this work, we present a healthy drink classiﬁer trained under a hierarchical multi-task learning framework composed of a shared residual network with hierarchically shared convolutional layers between similar tasks and task-speciﬁc fully-connected layers. The proposed structure includes two main tasks, namely sugar level classiﬁcation and alcoholic drink recognition, and six auxiliary tasks, such as classiﬁcation and recognition of drink name, drink type, branding logo, container transparency, container shape, and container material. We also curated a drink dataset, Drink101, composed of 101 different drinks including 11,445 images overall. Our experimental results demonstrate improved classiﬁcation precision compared to single-task learning and baseline multi-task learning approaches.},
language = {en},
urldate = {2020-01-29},
booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
publisher = {IEEE},
author = {Park, Homin and Bharadhwaj, Homanga and Lim, Brian Y.},
month = jul,
year = {2019},
pages = {1--8},
file = {Park et al. - 2019 - Hierarchical Multi-Task Learning for Healthy Drink.pdf:/home/konstaku/Zotero/storage/P49PC6F6/Park et al. - 2019 - Hierarchical Multi-Task Learning for Healthy Drink.pdf:application/pdf}
}

@INPROCEEDINGS{healthyDrink,

  author={H. {Park} and H. {Bharadhwaj} and B. Y. {Lim}},

  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)}, 

  title={Hierarchical Multi-Task Learning for Healthy Drink Classification}, 

  year={2019},

  volume={},

  number={},

  pages={1-8},}


@article{whatAndWhereToTransfer,
title = {Learning {What} and {Where} to {Transfer}},
url = {http://arxiv.org/abs/1905.05901},
abstract = {As the application of deep learning has expanded to real-world problems with insufﬁcient volume of training data, transfer learning recently has gained much attention as means of improving the performance in such small-data regime. However, when existing methods are applied between heterogeneous architectures and tasks, it becomes more important to manage their detailed conﬁgurations and often requires exhaustive tuning on them for the desired performance. To address the issue, we propose a novel transfer learning approach based on meta-learning that can automatically learn what knowledge to transfer from the source network to where in the target network. Given source and target networks, we propose an efﬁcient training scheme to learn meta-networks that decide (a) which pairs of layers between the source and target networks should be matched for knowledge transfer and (b) which features and how much knowledge from each feature should be transferred. We validate our meta-transfer approach against recent transfer learning methods on various datasets and network architectures, on which our automated scheme signiﬁcantly outperforms the prior baselines that ﬁnd “what and where to transfer” in a hand-crafted manner.},
language = {en},
urldate = {2019-12-09},
journal = {arXiv:1905.05901 [cs, stat]},
author = {Jang, Yunhun and Lee, Hankook and Hwang, Sung Ju and Shin, Jinwoo},
month = may,
year = {2019},
note = {arXiv: 1905.05901},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
file = {Jang et al. - 2019 - Learning What and Where to Transfer.pdf:/home/konstaku/Zotero/storage/264ZRMT9/Jang et al. - 2019 - Learning What and Where to Transfer.pdf:application/pdf}
}

@inproceedings{transferringMidLevelRepresentations,
address = {Columbus, OH, USA},
title = {Learning and {Transferring} {Mid}-level {Image} {Representations} {Using} {Convolutional} {Neural} {Networks}},
isbn = {978-1-4799-5118-5},
doi = {10.1109/CVPR.2014.222},
abstract = {Convolutional neural networks (CNN) have recently shown outstanding image classiﬁcation performance in the largescale visual recognition challenge (ILSVRC2012). The success of CNNs is attributed to their ability to learn rich midlevel image representations as opposed to hand-designed low-level features used in other image classiﬁcation methods. Learning CNNs, however, amounts to estimating millions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be efﬁciently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred representation leads to signiﬁcantly improved results for object and action classiﬁcation, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.},
language = {en},
booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
publisher = {IEEE},
author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
year = {2014},
pages = {1717--1724},
file = {Oquab et al. - 2014 - Learning and Transferring Mid-level Image Represen.pdf:/home/konstaku/Zotero/storage/3WWZQLSF/Oquab et al. - 2014 - Learning and Transferring Mid-level Image Represen.pdf:application/pdf}
}

@InProceedings{VGG,
  author       = "Karen Simonyan and Andrew Zisserman",
  title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
  booktitle    = "International Conference on Learning Representations",
  year         = "2015",
}

@inproceedings{multiTaskWeather,
address = {Mountain View, California, USA},
title = {A {Multi}-{Task} {Framework} for {Weather} {Recognition}},
isbn = {978-1-4503-4906-2},
doi = {10.1145/3123266.3123382},
abstract = {Weather recognition is important in practice, while this task has not been thoroughly explored so far. The current trend of dealing with this task is treating it as a single classi cation problem, i.e., determining whether a given image belongs to a certain weather category or not. However, weather recognition di ers signi cantly from traditional image classi cation, since several weather features may appear simultaneously. In this case, a simple classi cation result is insu cient to describe the weather condition. To address this issue, we propose to provide auxiliary weather related information for comprehensive weather description. Speci cally, semantic segmentation of weather-cues, such as blue sky and white clouds, is exploited as an auxiliary task in this paper. Moreover, a convolutional neural network (CNN) based multi-task framework is developed which aims to concurrently tackle weather category classi cation task and weather-cues segmentation task. Due to the intrinsic relationships between these two tasks, exploring auxiliary semantic segmentation of weather-cues can also help to learn discriminative features for the classi cation task, and thus obtain superior accuracy. To verify the e ectiveness of the proposed approach, extra segmentation masks of weather-cues are generated manually on an existing weather image dataset. Experimental results have demonstrated the superior performance of our approach. The enhanced dataset, source codes and pre-trained models are available at https://github.com/wzgwzg/Multitask\_Weather.},
language = {en},
urldate = {2019-12-06},
booktitle = {Proceedings of the 2017 {ACM} on {Multimedia} {Conference} - {MM} '17},
publisher = {ACM Press},
author = {Li, Xuelong and Wang, Zhigang and Lu, Xiaoqiang},
year = {2017},
pages = {1318--1326},
file = {Li et al. - 2017 - A Multi-Task Framework for Weather Recognition.pdf:/home/konstaku/Zotero/storage/SYNZBKS3/Li et al. - 2017 - A Multi-Task Framework for Weather Recognition.pdf:application/pdf}
}

@article{weatherNet,
title = {{WeatherNet}: {Recognising} {Weather} and {Visual} {Conditions} from {Street}-{Level} {Images} {Using} {Deep} {Residual} {Learning}},
volume = {8},
issn = {2220-9964},
shorttitle = {{WeatherNet}},
doi = {10.3390/ijgi8120549},
abstract = {Extracting information related to weather and visual conditions at a given time and space is indispensable for scene awareness, which strongly impacts our behaviours, from simply walking in a city to riding a bike, driving a car, or autonomous driveassistance. Despite the significance of this subject, it is still not been fully addressed by the machine intelligence relying on deep learning and computer vision to detect the multi-labels of weather and visual conditions with a unified method that can be easily used for practice. What has been achieved to-date is rather sectorial models that address limited number of labels that do not cover the wide spectrum of weather and visual conditions. Nonetheless, weather and visual conditions are often addressed individually. In this paper, we introduce a novel framework to automatically extract this information from street-level images relying on deep learning and computer vision using a unified method without any pre-defined constraints in the processed images. A pipeline of four deep Convolutional Neural Network (CNN) models, so-called the WeatherNet, is trained, relying on residual learning using ResNet50 architecture, to extract various weather and visual conditions such as Dawn/dusk, day and night for time detection, and glare for lighting conditions, and clear, rainy, snowy, and foggy for weather conditions. The WeatherNet shows strong performance in extracting this information from user-defined images or video streams that can be used not limited to: autonomous vehicles and drive-assistance systems, tracking behaviours, safety-related research, or even for better understanding cities through images for policy-makers.},
language = {en},
number = {12},
journal = {ISPRS International Journal of Geo-Information},
author = {Ibrahim, Mohamed R. and Haworth, James and Cheng, Tao},
month = nov,
year = {2019},
pages = {549},
file = {Ibrahim et al. - 2019 - WeatherNet Recognising Weather and Visual Conditi.pdf:/home/konstaku/Zotero/storage/V3IZJE9G/Ibrahim et al. - 2019 - WeatherNet Recognising Weather and Visual Conditi.pdf:application/pdf}
}
@ARTICLE{biologicalMultitask,

  author={W. {Zhang} and R. {Li} and T. {Zeng} and Q. {Sun} and S. {Kumar} and J. {Ye} and S. {Ji}},

  journal={IEEE Transactions on Big Data}, 

  title={Deep Model Based Transfer and Multi-Task Learning for Biological Image Analysis}, 

  year={2020},

  volume={6},

  number={2},

  pages={322-333},}


 @InProceedings{uberNet,
author = {Kokkinos, Iasonas},
title = {Ubernet: Training a Universal Convolutional Neural Network for Low-, Mid-, and High-Level Vision Using Diverse Datasets and Limited Memory},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2017},
pages = {6129-6138}
} 

@article{ruderOverview,
title = {An {Overview} of {Multi}-{Task} {Learning} in {Deep} {Neural} {Networks}},
url = {http://arxiv.org/abs/1706.05098},
abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
language = {en},
journal = {arXiv:1706.05098 [cs, stat]},
author = {Ruder, Sebastian},
year = {2017},
note = {arXiv: 1706.05098},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
file = {Ruder - 2017 - An Overview of Multi-Task Learning in Deep Neural .pdf:/home/konstaku/Zotero/storage/P935YLYB/Ruder - 2017 - An Overview of Multi-Task Learning in Deep Neural .pdf:application/pdf}
}

@incollection{multiPoseNet,
address = {Cham},
title = {{MultiPoseNet}: {Fast} {Multi}-{Person} {Pose} {Estimation} {Using} {Pose} {Residual} {Network}},
volume = {11215},
isbn = {978-3-030-01251-9 978-3-030-01252-6},
shorttitle = {{MultiPoseNet}},
abstract = {In this paper, we present MultiPoseNet, a novel bottom-up multi-person pose estimation architecture that combines a multi-task model with a novel assignment method. MultiPoseNet can jointly handle person detection, person segmentation and pose estimation problems. The novel assignment method is implemented by the Pose Residual Network (PRN) which receives keypoint and person detections, and produces accurate poses by assigning keypoints to person instances. On the COCO keypoints dataset, our pose estimation method outperforms all previous bottom-up methods both in accuracy (+4-point mAP over previous best result) and speed; it also performs on par with the best top-down methods while being at least 4x faster. Our method is the fastest real time system with ∼23 frames/sec.},
language = {en},
booktitle = {Computer {Vision} – {ECCV} 2018},
publisher = {Springer International Publishing},
author = {Kocabas, Muhammed and Karagoz, Salih and Akbas, Emre},
editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
year = {2018},
doi = {10.1007/978-3-030-01252-6_26},
note = {Series Title: Lecture Notes in Computer Science},
pages = {437--453},
file = {Kocabas et al. - 2018 - MultiPoseNet Fast Multi-Person Pose Estimation Us.pdf:/home/konstaku/Zotero/storage/EBABUMHZ/Kocabas et al. - 2018 - MultiPoseNet Fast Multi-Person Pose Estimation Us.pdf:application/pdf}
}




@misc{nvidiaRTX,
title = {Introducing {NVIDIA} {GeForce} {RTX} 2070 {Graphics} {Card}},
url = {https://www.nvidia.com/fi-fi/geforce/graphics-cards/rtx-2070/},
abstract = {Nauti reaaliaikaisesta säteenseurannasta, tekoälystä ja jopa kuusinkertaisesta suorituskyvystä edellisen sukupolven näytönohjaimiin verrattuna.{\textless}br/{\textgreater}},
language = {fi-fi},
urldate = {2020-03-25},
journal = {NVIDIA},
note = {Library Catalog: www.nvidia.com},
file = {Snapshot:/home/konstaku/Zotero/storage/TRZXWNM3/rtx-2070.html:text/html}
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@misc{Apex,
	title = {{NVIDIA}/apex},
	copyright = {BSD-3-Clause},
	url = {https://github.com/NVIDIA/apex},
	abstract = {A PyTorch Extension:  Tools for easy mixed precision and distributed training in Pytorch},
	urldate = {2020-03-25},
	publisher = {NVIDIA Corporation},
	year = {2020},
	note = {original-date: 2018-04-23T16:28:52Z}
}

@inproceedings{mixedTraining,
title={Mixed Precision Training},
author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
booktitle={International Conference on Learning Representations},
year={2018},
}

@misc{docker,
	title = {Empowering {App} {Development} for {Developers} {\textbar} {Docker}},
	url = {https://www.docker.com/},
	abstract = {Learn how Docker helps developers bring their ideas to life by conquering the complexity of app development.},
	language = {en},
	urldate = {2020-03-25},
	note = {Library Catalog: www.docker.com},
	file = {Snapshot:/home/konstaku/Zotero/storage/66KSUDIL/www.docker.com.html:text/html}
}
@ONLINE {beansdata,
	author="Makerere AI Lab",
	title="Bean disease dataset",
	year="2020",
	url="https://github.com/AI-Lab-Makerere/ibean/"
}


@article{citrusdata,
	title={A citrus fruits and leaves dataset for detection and classification of
			citrus diseases through machine learning},
	author={Rauf, Hafiz Tayyab and Saleem, Basharat Ali and Lali, M Ikram Ullah
			and Khan, Muhammad Attique and Sharif, Muhammad and Bukhari, Syed Ahmad Chan},
	journal={Data in brief},
	volume={26},
	pages={104340},
	year={2019},
	publisher={Elsevier}
}
@INPROCEEDINGS{cycliclr,

  author={L. N. {Smith}},

  booktitle={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 

  title={Cyclical Learning Rates for Training Neural Networks}, 

  year={2017},

  volume={},

  number={},

  pages={464-472}
  }

@incollection{attention,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
}


@article{cnn-rnn,
  author    = {Bin Zhao and
               Xuelong Li and
               Xiaoqiang Lu and
               Zhigang Wang},
  title     = {A {CNN-RNN} Architecture for Multi-Label Weather Recognition},
  journal   = {CoRR},
  volume    = {abs/1904.10709},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.10709},
  archivePrefix = {arXiv},
  eprint    = {1904.10709},
  timestamp = {Thu, 02 May 2019 15:13:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-10709.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{flowers102,
	author       = "Maria-Elena Nilsback and Andrew Zisserman",
	title        = "Automated Flower Classification over a Large Number of Classes",
	booktitle    = "Indian Conference on Computer Vision, Graphics and Image Processing",
	year         = "2008",
}

@article{self_attention_multi_task_object,
title = {{SpotNet}: {Self}-{Attention} {Multi}-{Task} {Network} for {Object} {Detection}},
shorttitle = {{SpotNet}},
url = {http://arxiv.org/abs/2002.05540},
abstract = {Humans are very good at directing their visual attention toward relevant areas when they search for different types of objects. For instance, when we search for cars, we will look at the streets, not at the top of buildings. The motivation of this paper is to train a network to do the same via a multi-task learning approach. To train visual attention, we produce foreground/background segmentation labels in a semi-supervised way, using background subtraction or optical ﬂow. Using these labels, we train an object detection model to produce foreground/background segmentation maps as well as bounding boxes while sharing most model parameters. We use those segmentation maps inside the network as a self-attention mechanism to weight the feature map used to produce the bounding boxes, decreasing the signal of non-relevant areas. We show that by using this method, we obtain a signiﬁcant mAP improvement on two trafﬁc surveillance datasets, with state-of-the-art results on both UA-DETRAC and UAVDT.},
language = {en},
urldate = {2020-05-06},
journal = {arXiv:2002.05540 [cs]},
author = {Perreault, Hughes and Bilodeau, Guillaume-Alexandre and Saunier, Nicolas and Héritier, Maguelonne},
month = feb,
year = {2020},
note = {arXiv: 2002.05540},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Perreault et al. - 2020 - SpotNet Self-Attention Multi-Task Network for Obj.pdf:/home/konstaku/Zotero/storage/6HYTMKUK/Perreault et al. - 2020 - SpotNet Self-Attention Multi-Task Network for Obj.pdf:application/pdf}
}

@inproceedings{self_supervised_object_detection,
address = {Long Beach, CA, USA},
title = {Multi-{Task} {Self}-{Supervised} {Object} {Detection} via {Recycling} of {Bounding} {Box} {Annotations}},
isbn = {978-1-72813-293-8},
doi = {10.1109/CVPR.2019.00512},
abstract = {In spite of recent enormous success of deep convolutional networks in object detection, they require a large amount of bounding box annotations, which are often timeconsuming and error-prone to obtain. To make better use of given limited labels, we propose a novel object detection approach that takes advantage of both multi-task learning (MTL) and self-supervised learning (SSL). We propose a set of auxiliary tasks that help improve the accuracy of object detection. They create their own labels by recycling the bounding box labels (i.e. annotations of the main task) in an SSL manner, and are jointly trained with the object detection model in an MTL way. Our approach is integrable with any region proposal based detection models. We empirically validate that our approach effectively improves detection performance on various architectures and datasets. We test two state-of-the-art region proposal object detectors, including Faster R-CNN [39] and R-FCN [10], with three CNN backbones of ResNet-101 [22], InceptionResNet-v2 [45], and MobileNet [23] on two benchmark datasets of PASCAL VOC [14] and COCO [30].},
language = {en},
urldate = {2020-05-06},
booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
publisher = {IEEE},
author = {Lee, Wonhee and Na, Joonil and Kim, Gunhee},
month = jun,
year = {2019},
pages = {4979--4988},
file = {Lee et al. - 2019 - Multi-Task Self-Supervised Object Detection via Re.pdf:/home/konstaku/Zotero/storage/LZ3IF86I/Lee et al. - 2019 - Multi-Task Self-Supervised Object Detection via Re.pdf:application/pdf}
}

@incollection{clothing_attributes_data,
address = {Berlin, Heidelberg},
title = {Describing {Clothing} by {Semantic} {Attributes}},
volume = {7574},
isbn = {978-3-642-33711-6 978-3-642-33712-3},
abstract = {Describing clothing appearance with semantic attributes is an appealing technique for many important applications. In this paper, we propose a fully automated system that is capable of generating a list of nameable attributes for clothes on human body in unconstrained images. We extract low-level features in a pose-adaptive manner, and combine complementary features for learning attribute classiﬁers. Mutual dependencies between the attributes are then explored by a Conditional Random Field to further improve the predictions from independent classiﬁers. We validate the performance of our system on a challenging clothing attribute dataset, and introduce a novel application of dressing style analysis that utilizes the semantic attributes produced by our system.},
language = {en},
urldate = {2020-05-06},
booktitle = {Computer {Vision} – {ECCV} 2012},
publisher = {Springer Berlin Heidelberg},
author = {Chen, Huizhong and Gallagher, Andrew and Girod, Bernd},
editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
year = {2012},
doi = {10.1007/978-3-642-33712-3_44},
note = {Series Title: Lecture Notes in Computer Science},
pages = {609--623},
file = {Chen et al. - 2012 - Describing Clothing by Semantic Attributes.pdf:/home/konstaku/Zotero/storage/6W5AP33A/Chen et al. - 2012 - Describing Clothing by Semantic Attributes.pdf:application/pdf}
}

@article{multi_task_attributes,
title = {Multi-task {CNN} {Model} for {Attribute} {Prediction}},
volume = {17},
issn = {1520-9210, 1941-0077},
url = {http://arxiv.org/abs/1601.00400},
doi = {10.1109/TMM.2015.2477680},
abstract = {This paper proposes a joint multi-task learning algorithm to better predict attributes in images using deep convolutional neural networks (CNN). We consider learning binary semantic attributes through a multi-task CNN model, where each CNN will predict one binary attribute. The multitask learning allows CNN models to simultaneously share visual knowledge among different attribute categories. Each CNN will generate attribute-speciﬁc feature representations, and then we apply multi-task learning on the features to predict their attributes. In our multi-task framework, we propose a method to decompose the overall model’s parameters into a latent task matrix and combination matrix. Furthermore, under-sampled classiﬁers can leverage shared statistics from other classiﬁers to improve their performance. Natural grouping of attributes is applied such that attributes in the same group are encouraged to share more knowledge. Meanwhile, attributes in different groups will generally compete with each other, and consequently share less knowledge. We show the effectiveness of our method on two popular attribute datasets.},
language = {en},
number = {11},
urldate = {2020-05-06},
journal = {IEEE Transactions on Multimedia},
author = {Abdulnabi, Abrar H. and Wang, Gang and Lu, Jiwen and Jia, Kui},
month = nov,
year = {2015},
note = {arXiv: 1601.00400},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
pages = {1949--1959},
file = {Abdulnabi et al. - 2015 - Multi-task CNN Model for Attribute Prediction.pdf:/home/konstaku/Zotero/storage/8QMCVZW5/Abdulnabi et al. - 2015 - Multi-task CNN Model for Attribute Prediction.pdf:application/pdf}
}

@article{cross_dataset_training,
title = {Cross-dataset {Training} for {Class} {Increasing} {Object} {Detection}},
url = {http://arxiv.org/abs/2001.04621},
abstract = {We present a conceptually simple, ﬂexible and general framework for cross-dataset training in object detection. Given two or more already labeled datasets that target for different object classes, cross-dataset training aims to detect the union of the different classes, so that we do not have to label all the classes for all the datasets. By crossdataset training, existing datasets can be utilized to detect the merged object classes with a single model. Further more, in industrial applications, the object classes usually increase on demand. So when adding new classes, it is quite time-consuming if we label the new classes on all the existing datasets. While using cross-dataset training, we only need to label the new classes on the new dataset. We experiment on PASCAL VOC, COCO, WIDER FACE and WIDER Pedestrian with both solo and cross-dataset settings. Results show that our cross-dataset pipeline can achieve similar impressive performance simultaneously on these datasets compared with training independently.},
language = {en},
journal = {arXiv:2001.04621 [cs]},
author = {Yao, Yongqiang and Wang, Yan and Guo, Yu and Lin, Jiaojiao and Qin, Hongwei and Yan, Junjie},
year = {2020},
note = {arXiv: 2001.04621},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Yao et al. - 2020 - Cross-dataset Training for Class Increasing Object.pdf:/home/konstaku/Zotero/storage/V6YRDQ9B/Yao et al. - 2020 - Cross-dataset Training for Class Increasing Object.pdf:application/pdf}
}

@inproceedings{viola-jones,
address = {Kauai, HI, USA},
title = {Rapid object detection using a boosted cascade of simple features},
volume = {1},
isbn = {978-0-7695-1272-3},
doi = {10.1109/CVPR.2001.990517},
abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The ﬁrst is the introduction of a new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efﬁcient classiﬁers[6]. The third contribution is a method for combining increasingly more complex classiﬁers in a “cascade” which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object speciﬁc focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
language = {en},
booktitle = {Proceedings of the 2001 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}. {CVPR} 2001},
publisher = {IEEE Comput. Soc},
author = {Viola, P. and Jones, M.},
year = {2001},
pages = {I--511--I--518},
file = {Viola and Jones - 2001 - Rapid object detection using a boosted cascade of .pdf:/home/konstaku/Zotero/storage/TYMPXNBF/Viola and Jones - 2001 - Rapid object detection using a boosted cascade of .pdf:application/pdf}
}

@inproceedings{hogs,
	address = {San Diego, CA, USA},
	title = {Histograms of {Oriented} {Gradients} for {Human} {Detection}},
	volume = {1},
	isbn = {978-0-7695-2372-9},
	doi = {10.1109/CVPR.2005.177},
	abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors signiﬁcantly outperform existing feature sets for human detection. We study the inﬂuence of each stage of the computation on performance, concluding that ﬁne-scale gradients, ﬁne orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
	language = {en},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	publisher = {IEEE},
	author = {Dalal, N. and Triggs, B.},
	year = {2005},
	pages = {886--893},
	file = {Dalal and Triggs - 2005 - Histograms of Oriented Gradients for Human Detecti.pdf:/home/konstaku/Zotero/storage/HKE72EBQ/Dalal and Triggs - 2005 - Histograms of Oriented Gradients for Human Detecti.pdf:application/pdf}
}

@incollection{faster-rcnn,
title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {91--99},
year = {2015},
publisher = {Curran Associates, Inc.},
}


@article{yolov4,
title = {{YOLOv4}: {Optimal} {Speed} and {Accuracy} of {Object} {Detection}},
shorttitle = {{YOLOv4}},
url = {http://arxiv.org/abs/2004.10934},
abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justiﬁcation of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of ∼65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet.},
language = {en},
journal = {arXiv:2004.10934 [cs, eess]},
author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
month = apr,
year = {2020},
note = {arXiv: 2004.10934},
keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
file = {Bochkovskiy et al. - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf:/home/konstaku/Zotero/storage/YFN9D2A4/Bochkovskiy et al. - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf:application/pdf}
}


@inproceedings{self_supervised_detection,
	location = {Long Beach, {CA}, {USA}},
	title = {Multi-Task Self-Supervised Object Detection via Recycling of Bounding Box Annotations},
	isbn = {978-1-72813-293-8},
	doi = {10.1109/CVPR.2019.00512},
	abstract = {In spite of recent enormous success of deep convolutional networks in object detection, they require a large amount of bounding box annotations, which are often timeconsuming and error-prone to obtain. To make better use of given limited labels, we propose a novel object detection approach that takes advantage of both multi-task learning ({MTL}) and self-supervised learning ({SSL}). We propose a set of auxiliary tasks that help improve the accuracy of object detection. They create their own labels by recycling the bounding box labels (i.e. annotations of the main task) in an {SSL} manner, and are jointly trained with the object detection model in an {MTL} way. Our approach is integrable with any region proposal based detection models. We empirically validate that our approach effectively improves detection performance on various architectures and datasets. We test two state-of-the-art region proposal object detectors, including Faster R-{CNN} [39] and R-{FCN} [10], with three {CNN} backbones of {ResNet}-101 [22], {InceptionResNet}-v2 [45], and {MobileNet} [23] on two benchmark datasets of {PASCAL} {VOC} [14] and {COCO} [30].},
	eventtitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {4979--4988},
	booktitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Lee, Wonhee and Na, Joonil and Kim, Gunhee},
	urldate = {2020-05-06},
	date = {2019-06},
	langid = {english},
	file = {Lee et al. - 2019 - Multi-Task Self-Supervised Object Detection via Re.pdf:/Users/kokutvon/Zotero/storage/LZ3IF86I/Lee et al. - 2019 - Multi-Task Self-Supervised Object Detection via Re.pdf:application/pdf}
}

@article{cross_data,
	title = {Cross-dataset Training for Class Increasing Object Detection},
	url = {http://arxiv.org/abs/2001.04621},
	abstract = {We present a conceptually simple, ﬂexible and general framework for cross-dataset training in object detection. Given two or more already labeled datasets that target for different object classes, cross-dataset training aims to detect the union of the different classes, so that we do not have to label all the classes for all the datasets. By crossdataset training, existing datasets can be utilized to detect the merged object classes with a single model. Further more, in industrial applications, the object classes usually increase on demand. So when adding new classes, it is quite time-consuming if we label the new classes on all the existing datasets. While using cross-dataset training, we only need to label the new classes on the new dataset. We experiment on {PASCAL} {VOC}, {COCO}, {WIDER} {FACE} and {WIDER} Pedestrian with both solo and cross-dataset settings. Results show that our cross-dataset pipeline can achieve similar impressive performance simultaneously on these datasets compared with training independently.},
	journaltitle = {{arXiv}:2001.04621 [cs]},
	author = {Yao, Yongqiang and Wang, Yan and Guo, Yu and Lin, Jiaojiao and Qin, Hongwei and Yan, Junjie},
	langid = {english},
	eprinttype = {arxiv},
	year=2020,
	eprint = {2001.04621},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Yao et al. - 2020 - Cross-dataset Training for Class Increasing Object.pdf:/Users/kokutvon/Zotero/storage/V6YRDQ9B/Yao et al. - 2020 - Cross-dataset Training for Class Increasing Object.pdf:application/pdf}
}

@article{PVOC,
	title = {The Pascal Visual Object Classes ({VOC}) Challenge},
	volume = {88},
	issn = {0920-5691, 1573-1405},
	doi = {10.1007/s11263-009-0275-4},
	abstract = {The {PASCAL} Visual Object Classes ({VOC}) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
	pages = {303--338},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	date = {2010-06},
	langid = {english},
	file = {Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:/Users/kokutvon/Zotero/storage/PL9ZPYH8/Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:application/pdf}
}

@online{COCO_SITE,
	title = {{COCO} - Common Objects in Context},
	url = {http://cocodataset.org/#detection-eval},
	urldate = {2020-05-18},
	file = {COCO - Common Objects in Context:/Users/kokutvon/Zotero/storage/2BRKN5AW/cocodataset.org.html:text/html}
}

@InProceedings{COCO,
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollar, Piotr and Zitnick, Larry},
title = {Microsoft COCO: Common Objects in Context},
booktitle = {ECCV},
year = {2014},
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
publisher = {European Conference on Computer Vision},
edition = {ECCV},
}

@inproceedings{retinaNet,
  author={T. {Lin} and P. {Goyal} and R. {Girshick} and K. {He} and P. {Dollár}},

  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 

  title={Focal Loss for Dense Object Detection}, 

  year={2017},

  volume={},

  number={},

  pages={2999-3007},}


@article{SPP,
  author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}, 

  year={2015},

  volume={37},

  number={9},

  pages={1904-1916}
  }


@inproceedings{PANET,
 author={S. {Liu} and L. {Qi} and H. {Qin} and J. {Shi} and J. {Jia}},

  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 

  title={Path Aggregation Network for Instance Segmentation}, 

  year={2018},

  volume={},

  number={},

  pages={8759-8768}
}


@inproceedings{efficientDet,
title	= {EfficientDet: Scalable and Efficient Object Detection},
author	= {Mingxing Tan and Ruoming Pang and Quoc V. Le},
year	= {2020},
URL	= {https://arxiv.org/abs/1911.09070}
}




@inproceedings{FPN,
  author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross B. and He, Kaiming and Hariharan, Bharath and Belongie, Serge J.},
  biburl = {https://www.bibsonomy.org/bibtex/2648eac01a787fc806e44efd5b5c617f9/slicside},
  booktitle = {CVPR},
  crossref = {conf/cvpr/2017},
  ee = {http://doi.ieeecomputersociety.org/10.1109/CVPR.2017.106},
  isbn = {978-1-5386-0457-1},
  pages = {936-944},
  publisher = {IEEE Computer Society},
  title = {Feature Pyramid Networks for Object Detection.},
  year = 2017
}


@book{deep-learning,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@InProceedings{catsdogs,
  author       = "Omkar M. Parkhi and Andrea Vedaldi and Andrew Zisserman and C. V. Jawahar",
  title        = "Cats and Dogs",
  booktitle    = "IEEE Conference on Computer Vision and Pattern Recognition",
  year         = "2012",
}
@software{pytorch-efficientdet,
	title = {zylo117/Yet-Another-{EfficientDet}-Pytorch},
	rights = {{LGPL}-3.0},
	url = {https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch},
	abstract = {The pytorch re-implement of the official efficientdet with {SOTA} performance in real time and pretrained weights.},
	author = {zylo117},
	urldate = {2020-05-26},
	year = {2020},
	keywords = {bifpn, detection, efficientdet, efficientnet, object-detection, pytorch}
}

@inproceedings{multinet,
	title = {{MultiNet}: Real-time Joint Semantic Reasoning for Autonomous Driving},
	doi = {10.1109/IVS.2018.8500504},
	shorttitle = {{MultiNet}},
	abstract = {While most approaches to semantic reasoning have focused on improving performance, in this paper we argue that computational times are very important in order to enable real time applications such as autonomous driving. Towards this goal, we present an approach to joint classification, detection and semantic segmentation using a unified architecture where the encoder is shared amongst the three tasks. Our approach is very simple, can be trained end-to-end and performs extremely well in the challenging {KITTI} dataset. Our approach is also very efficient, allowing us to perform inference at more then 23 frames per second. Training scripts and trained weights to reproduce our results can be found here: https://github.com/{MarvinTeichmann}/{MultiNet}.},
	eventtitle = {2018 {IEEE} Intelligent Vehicles Symposium ({IV})},
	pages = {1013--1020},
	booktitle = {2018 {IEEE} Intelligent Vehicles Symposium ({IV})},
	author = {Teichmann, Marvin and Weber, Michael and Zöllner, Marius and Cipolla, Roberto and Urtasun, Raquel},
	date = {2018-06},
	note = {{ISSN}: 1931-0587},
	keywords = {autonomous driving, Computer architecture, Decoding, driver information systems, Feature extraction, image classification, image segmentation, inference mechanisms, joint classification, {KITTI} dataset, Microprocessors, {MultiNet}, object detection, Proposals, real-time joint semantic reasoning, semantic segmentation, Semantics, Task analysis, unified architecture},
	file = {IEEE Xplore Abstract Record:/Users/kokutvon/Zotero/storage/HFMRMY8D/8500504.html:text/html;IEEE Xplore Full Text PDF:/Users/kokutvon/Zotero/storage/TJDD6225/Teichmann et al. - 2018 - MultiNet Real-time Joint Semantic Reasoning for A.pdf:application/pdf}
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@INPROCEEDINGS{nms,

  author={N. {Bodla} and B. {Singh} and R. {Chellappa} and L. S. {Davis}},

  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 

  title={Soft-NMS — Improving Object Detection with One Line of Code}, 

  year={2017},

  volume={},

  number={},

  pages={5562-5570},}


@InProceedings{Simonyan15,
  author       = "Karen Simonyan and Andrew Zisserman",
  title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
  booktitle    = "International Conference on Learning Representations",
  year         = "2015",
}

@INPROCEEDINGS{vbow,

  author={A. {Faheema} and S. {Rakshit}},

  booktitle={2010 IEEE 2nd International Advance Computing Conference (IACC)}, 

  title={Feature selection using bag-of-visual-words representation}, 

  year={2010},

  volume={},

  number={},

  pages={151-156},}
