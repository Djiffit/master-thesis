@article{einstein,
author =       "Albert Einstein",
title =        "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
[{On} the electrodynamics of moving bodies]",
journal =      "Annalen der Physik",
volume =       "322",
number =       "10",
pages =        "891--921",
year =         "1905",
DOI =          "http://dx.doi.org/10.1002/andp.19053221004"
}

@article{guerra_weather_2018,
title = {Weather {Classification}: {A} new multi-class dataset, data augmentation approach and comprehensive evaluations of {Convolutional} {Neural} {Networks}},
shorttitle = {Weather {Classification}},
url = {http://arxiv.org/abs/1808.00588},
abstract = {Weather conditions often disrupt the proper functioning of transportation systems. Present systems either deploy an array of sensors or use an in-vehicle camera to predict weather conditions. These solutions have resulted in incremental cost and limited scope. To ensure smooth operation of all transportation services in all-weather conditions, a reliable detection system is necessary to classify weather in wild. The challenges involved in solving this problem is that weather conditions are diverse in nature and there is an absence of discriminate features among various weather conditions. The existing works to solve this problem have been scene speciﬁc and have targeted classiﬁcation of two categories of weather. In this paper, we have created a new open source dataset consisting of images depicting three classes of weather i.e rain, snow and fog called RFS Dataset. A novel algorithm has also been proposed which has used super pixel delimiting masks as a form of data augmentation, leading to reasonable results with respect to ten Convolutional Neural Network architectures.},
language = {en},
urldate = {2019-12-03},
journal = {arXiv:1808.00588 [cs]},
author = {Guerra, Jose Carlos Villarreal and Khanam, Zeba and Ehsan, Shoaib and Stolkin, Rustam and McDonald-Maier, Klaus},
month = aug,
year = {2018},
note = {arXiv: 1808.00588},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Guerra et al. - 2018 - Weather Classification A new multi-class dataset,.pdf:/Users/kokutvon/Zotero/storage/69INUZPT/Guerra et al. - 2018 - Weather Classification A new multi-class dataset,.pdf:application/pdf}
}

@article{ibrahim_weathernet:_2019,
title = {{WeatherNet}: {Recognising} {Weather} and {Visual} {Conditions} from {Street}-{Level} {Images} {Using} {Deep} {Residual} {Learning}},
volume = {8},
issn = {2220-9964},
shorttitle = {{WeatherNet}},
url = {https://www.mdpi.com/2220-9964/8/12/549},
doi = {10.3390/ijgi8120549},
abstract = {Extracting information related to weather and visual conditions at a given time and space is indispensable for scene awareness, which strongly impacts our behaviours, from simply walking in a city to riding a bike, driving a car, or autonomous driveassistance. Despite the significance of this subject, it is still not been fully addressed by the machine intelligence relying on deep learning and computer vision to detect the multi-labels of weather and visual conditions with a unified method that can be easily used for practice. What has been achieved to-date is rather sectorial models that address limited number of labels that do not cover the wide spectrum of weather and visual conditions. Nonetheless, weather and visual conditions are often addressed individually. In this paper, we introduce a novel framework to automatically extract this information from street-level images relying on deep learning and computer vision using a unified method without any pre-defined constraints in the processed images. A pipeline of four deep Convolutional Neural Network (CNN) models, so-called the WeatherNet, is trained, relying on residual learning using ResNet50 architecture, to extract various weather and visual conditions such as Dawn/dusk, day and night for time detection, and glare for lighting conditions, and clear, rainy, snowy, and foggy for weather conditions. The WeatherNet shows strong performance in extracting this information from user-defined images or video streams that can be used not limited to: autonomous vehicles and drive-assistance systems, tracking behaviours, safety-related research, or even for better understanding cities through images for policy-makers.},
language = {en},
number = {12},
urldate = {2019-12-03},
journal = {IJGI},
author = {Ibrahim, Mohamed R. and Haworth, James and Cheng, Tao},
month = nov,
year = {2019},
pages = {549},
file = {Ibrahim et al. - 2019 - WeatherNet Recognising Weather and Visual Conditi.pdf:/Users/kokutvon/Zotero/storage/V3IZJE9G/Ibrahim et al. - 2019 - WeatherNet Recognising Weather and Visual Conditi.pdf:application/pdf}
}

@article{pan_winter_nodate,
title = {Winter {Road} {Surface} {Condition} {Recognition} {Using} a {Pre}-trained {Deep} {Convolutional} {Neural} {Network}},
language = {en},
author = {Pan, Guangyuan and Fu, Liping and Yu, Ruifan and Muresan, Matthew},
pages = {13},
file = {Pan et al. - Winter Road Surface Condition Recognition Using a .pdf:/Users/kokutvon/Zotero/storage/AQWPYSCB/Pan et al. - Winter Road Surface Condition Recognition Using a .pdf:application/pdf}
}
@book{latexcompanion,
	author    = "Michel Goossens and Frank Mittelbach and Alexander Samarin",
	title     = "The \LaTeX\ Companion",
	year      = "1993",
	publisher = "Addison-Wesley",
	address   = "Reading, Massachusetts"
}

@book{knuth99,
	author    = "Donald E. Knuth",
	title     = "Digital Typography",
	year      = "1999",
	publisher = "The Center for the Study of Language and Information",
	series    = "CLSI Lecture Notes (78)"
}

@misc{knuthwebsite,
	author    = "Donald Knuth",
	title     = "Knuth: Computers and Typesetting",
	url       = "http://www-cs-faculty.stanford.edu/~knuth/abcde.html"
}

@TECHREPORT{erkio01,
	AUTHOR  = "Erkiö, H. and Mäkelä, M. and Nykänen, M. and Verkamo, I.",
	TITLE   = "Opinnäytetyän ulkoasun malli.",
	INSTITUTION = "Tietojenkäsittelyopin laitos",
	YEAR        = 2001,
	TYPE        = "Tieteellisen kirjoittamisen kurssiin
	liittyvä julkaisematon moniste",
	ADDRESS     = "Helsinki"
}



@ARTICLE{crowdenetal79,
AUTHOR  = "Crowder, H. and Dembo, R.S. and Mulvey, J.M.",
TITLE   = "On reporting computational experiments
with mathematical software",
JOURNAL = "{ACM} Transactions on Mathematical Software",
YEAR    = 1979,
VOLUME  = 5,
NUMBER  = 2,
PAGES   = "193--203"
}

@INPROCEEDINGS{dantowsley90,
AUTHOR       = "Dan, A. and Towsley, D.",
TITLE        = "An approximate analysis of the {LRU} and {FIFO}
buffer replacement schemes",
BOOKTITLE    = "Proc. {ACM} Conf.\ Measurement and
Modeling of Computer Systems",
YEAR         = 1990,
PAGES        = "143--152",
ADDRESS      = "Boulder, Colorado, {USA}",
MONTH        = may
}

@TECHREPORT{erkio94,
	AUTHOR      = "Erkiö, H.",
	TITLE   = "Opinnäytetyän ulkoasun malli.",
	INSTITUTION = "Tietojenkäsittelyopin laitos",
	YEAR        = 1994,
	TYPE        = "Tieteellisen kirjoittamisen kurssiin
	liittyvä julkaisematon moniste",
	ADDRESS     = "Helsinki"
}

@TECHREPORT{erkiomakela96,
	AUTHOR      = "Erkiö, H. and Mäkelä, M.",
	TITLE       = "Opinnäytetyän ulkoasun malli",
	INSTITUTION = "Tietojenkäsittelytieteen laitos",
	YEAR        = 1996,
	TYPE        = "Tieteellisen kirjoittamisen kurssiin
	liittyvä julkaisematon moniste",
	ADDRESS     = "Helsinki"
}

@BOOK{fogelbergetal89,
	AUTHOR    = "Fogelberg, P. and Herranen, M. and Sinikara, K.",
	TITLE     = "Tuumasta toimeen, tutkielman tekijän opas",
	PUBLISHER = "Yliopistopaino",
	YEAR      = 1989,
	ADDRESS   = "Helsinki"
}

@INCOLLECTION{gannonetal89,
	AUTHOR    = "Gannon, D. and others",
	TITLE     = "Programming environments for parallel algorithms",
	BOOKTITLE = "Parallel and Distributed Algorithms",
	YEAR      = 1989,
	EDITOR    = "M. Cosnard and others",
	publisher    = "North-Holland",
	PAGES     = "101--108"
}

@BOOK{grimm87,
	AUTHOR    = "Grimm, S. S.",
	TITLE     = "How to write computer documentation for users",
	PUBLISHER = "Van Nostrand Reinhold Co.",
	YEAR      = 1987,
	ADDRESS   = "New York"
}

@BOOK{harkinsplung82,
	EDITOR    = "Harkins, C. and Plung, D. L.",
	TITLE     = "A guide for writing better technical papers",
	PUBLISHER = "IEEE Press",
	YEAR      = 1982}

@TECHREPORT{julkohj81,
AUTHOR      = "{Helsingin yliopisto}",
TITLE       = "Julkaisusarjoja ja opinnäytteiden tiivistelmiä
koskevat ohjeet ja suositukset",
YEAR        = 1981,
TYPE        = "Helsingin yliopiston kirjastolaitoksen julkaisu {A 3}",
ADDRESS     = "Helsinki"
}

@WWW-MISC{kilpelainen00,
AUTHOR       = "Kilpeläinen, P.",
TITLE        = "{WWW}-lähteisiin viittaaminen tutkielmatekstissä",
URL          = "http://www.cs.helsinki.fi/u/kilpelai/TiKi/wwwlahteet.html",
YEAR         =  2000,
TYPE        = "Tieteellisen kirjoittamisen kurssiin
liittyvä julkaisematon moniste",
VALID        =  "19.1.2000"
}

@ARTICLE{smith78a,
AUTHOR  = "Smith, A. J.",
TITLE   = "Sequentiality and prefetching in database systems",
JOURNAL = "{ACM} Transactions on Database Systems",
YEAR    = 1978,
VOLUME  = 3,
NUMBER  = 3,
PAGES   = "223--247"
}

@ARTICLE{smith78b,
	AUTHOR  = "Smith, A. J.",
	TITLE   = "Sequential program prefetching in memory hierarchies",
	JOURNAL = "Computer",
	YEAR    = 1978,
	VOLUME  = 11,
	NUMBER  = 11,
	PAGES   = "7--21"
}

@TECHREPORT{verkamo92,
	AUTHOR      = "Verkamo, A. I.",
	TITLE       = "Opinnäytetyän ulkoasun malli",
	INSTITUTION = "Tietojenkäsittelyopin laitos",
	YEAR        = 1992,
	TYPE        = "Tieteellisen kirjoittamisen kurssiin
	liittyvä julkaisematon moniste",
	ADDRESS     = "Helsinki"
}

@ARTICLE{abiteboul,

AUTHOR  = "Abiteboul, S. and Quass, D. and McHugh, J. and
Widom, J. and Wiener, J.L.",
TITLE   = "The Lorel query language for semistructured data",
JOURNAL = "International Journal on Digital Libraries",
YEAR    = 1997,
VOLUME  = 1,
NUMBER  = 1,
PAGES   = "68-88",
NOTE    = "[
\url{http://link.springer.de/
	link/service/journals/00799/
	bibs/7001001/70010068.htm},
18.1.2000]"
}


@misc{bray,
AUTHOR = "Bray, T. and Paoli, J. and Sperberg-McQueen, C.M.",
TITLE  = "{Extensible Markup Language {XML} 1.0. W3C Recommendation 10-February-1998}",
howpublished    = "[\url{http://www.w3.org/TR/1998/REC-xml-19980210}, 02.11.2016]"
}

@ARTICLE{dietinger,

AUTHOR  = "Dietinger, T. and others",
TITLE   = "{Dynamic Background Libraries - New Developments in
Distance Education Using {HIKS} {Hierarchical Interactive
		Knowledge System}}",
JOURNAL = "Journal of Universal Computer Science",
YEAR    = 1999,
VOLUME  = 5,
NUMBER  = 1,
NOTE    = "[\url{http://www.iicm.edu/
	jucs_5_1/dynamic_background_libraries_new},
18.1.2000]"
}

@article{alexNet,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {00010782},
	url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2019-12-06},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/home/konstaku/Zotero/storage/45HSR2RH/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf}
}

@article{imagenet,
title = {{ImageNet}: {A} {Large}-{Scale} {Hierarchical} {Image} {Database}},
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
language = {en},
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
pages = {8},
year = {2010},
file = {Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf:/home/konstaku/Zotero/storage/F5WWKAJK/Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf:application/pdf}
}


@misc{ILSVRC,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Competition} ({ILSVRC})},
	url = {http://www.image-net.org/challenges/LSVRC/},
	urldate = {2020-01-26},
	file = {ImageNet Large Scale Visual Recognition Competition (ILSVRC):/home/konstaku/Zotero/storage/2IGV2WF3/LSVRC.html:text/html}
}

@article{resNet,
title = {Deep {Residual} {Learning} for {Image} {Recognition}},
url = {http://arxiv.org/abs/1512.03385},
abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
language = {en},
urldate = {2019-12-06},
journal = {arXiv:1512.03385 [cs]},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
month = dec,
year = {2015},
note = {arXiv: 1512.03385},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
annote = {Comment: Tech report},
file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/home/konstaku/Zotero/storage/CMC3Y7YX/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@article{imageNet_summary,
title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
url = {http://arxiv.org/abs/1409.0575},
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classiﬁcation and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than ﬁfty institutions.},
language = {en},
urldate = {2020-01-26},
journal = {arXiv:1409.0575 [cs]},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
month = jan,
year = {2015},
note = {arXiv: 1409.0575},
keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.8, I.5.2},
file = {Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:/home/konstaku/Zotero/storage/7U9W6FSF/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:application/pdf}
}

@article{objectNet,
title = {{ObjectNet}: {A} large-scale bias-controlled dataset for pushing the limits of object recognition models},
abstract = {We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientiﬁc experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be ﬁne-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45\% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to ﬁne-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet – objects are largely centered and unoccluded – and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.},
language = {en},
year = {2019},
author = {Barbu, Andrei and Mayo, David and Alverio, Julian and Luo, William and Wang, Christopher and Gutfreund, Dan and Tenenbaum, Josh and Katz, Boris},
pages = {11},
file = {Barbu et al. - ObjectNet A large-scale bias-controlled dataset f.pdf:/home/konstaku/Zotero/storage/QIHM7LV9/Barbu et al. - ObjectNet A large-scale bias-controlled dataset f.pdf:application/pdf}
}
@article{efficientNet,
title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
shorttitle = {{EfficientNet}},
url = {http://arxiv.org/abs/1905.11946},
abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a ﬁxed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefﬁcient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.},
language = {en},
urldate = {2019-12-09},
journal = {arXiv:1905.11946 [cs, stat]},
author = {Tan, Mingxing and Le, Quoc V.},
month = nov,
year = {2019},
note = {arXiv: 1905.11946},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
file = {Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf:/home/konstaku/Zotero/storage/IA4SQF8Z/Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf}
}

@article{betterTransfer,
title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
url = {http://arxiv.org/abs/1805.08974},
abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classiﬁcation networks on 12 image classiﬁcation datasets. We ﬁnd that, when networks are used as ﬁxed feature extractors or ﬁne-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we ﬁnd that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we ﬁnd that, on two small ﬁne-grained image classiﬁcation datasets, pretraining on ImageNet provides minimal beneﬁts, indicating the learned features from ImageNet do not transfer well to ﬁne-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
language = {en},
urldate = {2020-01-27},
journal = {arXiv:1805.08974 [cs, stat]},
author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
month = jun,
year = {2019},
note = {arXiv: 1805.08974},
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
file = {Kornblith et al. - 2020 - Do Better ImageNet Models Transfer Better.pdf:/home/konstaku/Zotero/storage/XYMF3C3M/Kornblith et al. - 2019 - Do Better ImageNet Models Transfer Better.pdf:application/pdf}
}

@article{classifierPerformance,
title = {Compounding the {Performance} {Improvements} of {Assembled} {Techniques} in a {Convolutional} {Neural} {Network}},
url = {http://arxiv.org/abs/2001.06268},
abstract = {Recent studies in image classiﬁcation have demonstrated a variety of techniques for improving the performance of Convolutional Neural Networks (CNNs). However, attempts to combine existing techniques to create a practical model are still uncommon. In this study, we carry out extensive experiments to validate that carefully assembling these techniques and applying them to a basic CNN model in combination can improve the accuracy and robustness of the model while minimizing the loss of throughput. For example, our proposed ResNet-50 shows an improvement in top-1 accuracy from 76.3\% to 82.78\%, and an mCE improvement from 76.0\% to 48.9\%, on the ImageNet ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. The resulting model signiﬁcantly outperforms state-of-theart models with similar accuracy in terms of mCE and inference throughput. To verify the performance improvement in transfer learning, ﬁne grained classiﬁcation and image retrieval tasks were tested on several open datasets and showed that the improvement to backbone network performance boosted transfer learning performance signiﬁcantly. Our approach achieved 1st place in the iFood Competition Fine-Grained Visual Recognition at CVPR 2019 1, and the source code and trained models will be made publicly available 2.},
language = {en},
urldate = {2020-01-27},
journal = {arXiv:2001.06268 [cs]},
author = {Lee, Jungkyu and Won, Taeryun and Hong, Kiho},
month = jan,
year = {2020},
note = {arXiv: 2001.06268},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Lee et al. - 2020 - Compounding the Performance Improvements of Assemb.pdf:/home/konstaku/Zotero/storage/788PI52C/Lee et al. - 2020 - Compounding the Performance Improvements of Assemb.pdf:application/pdf}
}

@article{leNet,
title = {Gradient-{Based} {Learning} {Applied} to {Document} {Recognition}},
language = {en},
author = {LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Ha, Patrick},
year = {1998},
pages = {46},
file = {LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf:/home/konstaku/Zotero/storage/UISSJ64F/LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf:application/pdf}
}

@article{rethinkTransfer,
title = {Rethinking {ImageNet} {Pre}-training},
url = {http://arxiv.org/abs/1811.08883},
abstract = {We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for ﬁne-tuning pretrained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when: (i) using only 10\% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve ﬁnal target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data—a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of ‘pretraining and ﬁne-tuning’ in computer vision.},
language = {en},
urldate = {2020-04-01},
journal = {arXiv:1811.08883 [cs]},
author = {He, Kaiming and Girshick, Ross and Dollár, Piotr},
month = nov,
year = {2018},
note = {arXiv: 1811.08883},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {He et al. - 2018 - Rethinking ImageNet Pre-training.pdf:/home/konstaku/Zotero/storage/MTNUR8KT/He et al. - 2018 - Rethinking ImageNet Pre-training.pdf:application/pdf}
}

@inproceedings{transferSurvey,
title = {A {Survey} of {Transfer} {Learning} for {Convolutional} {Neural} {Networks}},
doi = {10.1109/SIBGRAPI-T.2019.00010},
abstract = {Transfer learning is an emerging topic that may drive the success of machine learning in research and industry. The lack of data on specific tasks is one of the main reasons to use it, since collecting and labeling data can be very expensive and can take time, and recent concerns with privacy make difficult to use real data from users. The use of transfer learning helps to fast prototype new machine learning models using pre-trained models from a source task since training on millions of images can take time and requires expensive GPUs. In this survey, we review the concepts and definitions related to transfer learning and we list the different terms used in the literature. We bring the point of view from different authors of prior surveys, adding some more recent findings in order to give a clear vision of directions for future work in this field of research.},
booktitle = {2019 32nd {SIBGRAPI} {Conference} on {Graphics}, {Patterns} and {Images} {Tutorials} ({SIBGRAPI}-{T})},
author = {Ribani, Ricardo and Marengoni, Mauricio},
month = oct,
year = {2019},
note = {ISSN: 2474-0691},
keywords = {Machine learning, Convolutional Neural Networks, Data models, Deep Learning, Probability distribution, Supervised learning, Task analysis, Training, Transfer Learning},
pages = {47--57},
file = {IEEE Xplore Abstract Record:/home/konstaku/Zotero/storage/ZCESIJS9/8920338.html:text/html;IEEE Xplore Full Text PDF:/home/konstaku/Zotero/storage/PZN27DVW/Ribani and Marengoni - 2019 - A Survey of Transfer Learning for Convolutional Ne.pdf:application/pdf}
}

@article{transferSurvey2010,
	title = {A {Survey} on {Transfer} {Learning}},
	volume = {22},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5288526/},
	doi = {10.1109/TKDE.2009.191},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classiﬁcation task in one domain of interest, but we only have sufﬁcient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classiﬁcation, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
	language = {en},
	number = {10},
	urldate = {2020-01-29},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	month = oct,
	year = {2010},
	pages = {1345--1359},
	file = {Pan and Yang - 2010 - A Survey on Transfer Learning.pdf:/home/konstaku/Zotero/storage/YZBBSKDB/Pan and Yang - 2010 - A Survey on Transfer Learning.pdf:application/pdf}
}

@article{denseNet,
title = {Densely {Connected} {Convolutional} {Networks}},
url = {http://arxiv.org/abs/1608.06993},
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
language = {en},
urldate = {2020-02-19},
journal = {arXiv:1608.06993 [cs]},
author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
month = jan,
year = {2018},
note = {arXiv: 1608.06993},
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
file = {Huang et al. - 2018 - Densely Connected Convolutional Networks.pdf:/home/konstaku/Zotero/storage/58JA8UJ2/Huang et al. - 2018 - Densely Connected Convolutional Networks.pdf:application/pdf}
}

@article{resNext,
title = {Aggregated {Residual} {Transformations} for {Deep} {Neural} {Networks}},
url = {http://arxiv.org/abs/1611.05431},
abstract = {We present a simple, highly modularized network architecture for image classiﬁcation. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classiﬁcation accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classiﬁcation task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online1.},
language = {en},
urldate = {2020-02-19},
journal = {arXiv:1611.05431 [cs]},
author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
month = apr,
year = {2017},
note = {arXiv: 1611.05431},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Xie et al. - 2017 - Aggregated Residual Transformations for Deep Neura.pdf:/home/konstaku/Zotero/storage/7VQBC36Y/Xie et al. - 2017 - Aggregated Residual Transformations for Deep Neura.pdf:application/pdf}
}
@article{wideResNet,
title = {Wide {Residual} {Networks}},
url = {http://arxiv.org/abs/1605.07146},
abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efﬁciency all previous deep residual networks, including thousand-layerdeep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and signiﬁcant improvements on ImageNet. Our code and models are available at https: //github.com/szagoruyko/wide-residual-networks.},
language = {en},
urldate = {2020-02-19},
journal = {arXiv:1605.07146 [cs]},
author = {Zagoruyko, Sergey and Komodakis, Nikos},
month = jun,
year = {2017},
note = {arXiv: 1605.07146},
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
file = {Zagoruyko and Komodakis - 2017 - Wide Residual Networks.pdf:/home/konstaku/Zotero/storage/QE833XZI/Zagoruyko and Komodakis - 2017 - Wide Residual Networks.pdf:application/pdf}
}

@article{mobileNetv2,
title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
shorttitle = {{MobileNetV2}},
url = {http://arxiv.org/abs/1801.04381},
abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efﬁcient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.},
language = {en},
urldate = {2020-02-19},
journal = {arXiv:1801.04381 [cs]},
author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
month = mar,
year = {2019},
note = {arXiv: 1801.04381},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:/home/konstaku/Zotero/storage/I4FRFPZ4/Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:application/pdf}
}

@article{gPipe,
title = {{GPipe}: {Efficient} {Training} of {Giant} {Neural} {Networks} using {Pipeline} {Parallelism}},
shorttitle = {{GPipe}},
url = {http://arxiv.org/abs/1811.06965},
abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-speciﬁc and do not transfer to other tasks. To address the need for efﬁcient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the ﬂexibility of scaling a variety of different networks to gigantic sizes efﬁciently. Moreover, GPipe utilizes a novel batchsplitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classiﬁcation: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4\% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
language = {en},
urldate = {2020-02-21},
journal = {arXiv:1811.06965 [cs]},
author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
month = jul,
year = {2019},
note = {arXiv: 1811.06965},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Huang et al. - 2019 - GPipe Efficient Training of Giant Neural Networks.pdf:/home/konstaku/Zotero/storage/R5R53XUM/Huang et al. - 2019 - GPipe Efficient Training of Giant Neural Networks.pdf:application/pdf}
}


@article{mobileNet,
title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
shorttitle = {{MobileNets}},
url = {http://arxiv.org/abs/1704.04861},
abstract = {We present a class of efﬁcient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyperparameters that efﬁciently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classiﬁcation. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, ﬁnegrain classiﬁcation, face attributes and large scale geo-localization.},
language = {en},
urldate = {2020-02-21},
journal = {arXiv:1704.04861 [cs]},
author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
month = apr,
year = {2017},
note = {arXiv: 1704.04861},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:/home/konstaku/Zotero/storage/DGE2QG6B/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf}
}


@article{wideResNet,
title = {Wide {Residual} {Networks}},
url = {http://arxiv.org/abs/1605.07146},
abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efﬁciency all previous deep residual networks, including thousand-layerdeep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and signiﬁcant improvements on ImageNet. Our code and models are available at https: //github.com/szagoruyko/wide-residual-networks.},
language = {en},
urldate = {2020-02-21},
journal = {arXiv:1605.07146 [cs]},
author = {Zagoruyko, Sergey and Komodakis, Nikos},
month = jun,
year = {2017},
note = {arXiv: 1605.07146},
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
file = {Zagoruyko and Komodakis - 2017 - Wide Residual Networks.pdf:/home/konstaku/Zotero/storage/PCVBZZQY/Zagoruyko and Komodakis - 2017 - Wide Residual Networks.pdf:application/pdf}
}

@article{mnas,
title = {{MnasNet}: {Platform}-{Aware} {Neural} {Architecture} {Search} for {Mobile}},
shorttitle = {{MnasNet}},
url = {http://arxiv.org/abs/1807.11626},
abstract = {Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although signiﬁcant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difﬁcult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between ﬂexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classiﬁcation task, our MnasNet achieves 75.2\% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8× faster than MobileNetV2 [29] with 0.5\% higher accuracy and 2.3× faster than NASNet [36] with 1.2\% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/ tree/master/models/official/mnasnet.},
language = {en},
urldate = {2020-02-21},
journal = {arXiv:1807.11626 [cs]},
author = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V.},
month = may,
year = {2019},
note = {arXiv: 1807.11626},
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
file = {Tan et al. - 2019 - MnasNet Platform-Aware Neural Architecture Search.pdf:/home/konstaku/Zotero/storage/B2E83S43/Tan et al. - 2019 - MnasNet Platform-Aware Neural Architecture Search.pdf:application/pdf}
}

@article{neuralSearch,
title = {Neural {Architecture} {Search}: {A} {Survey}},
shorttitle = {Neural {Architecture} {Search}},
url = {http://arxiv.org/abs/1808.05377},
abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this ﬁeld of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
language = {en},
urldate = {2020-02-21},
journal = {arXiv:1808.05377 [cs, stat]},
author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
month = apr,
year = {2019},
note = {arXiv: 1808.05377},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
file = {Elsken et al. - 2019 - Neural Architecture Search A Survey.pdf:/home/konstaku/Zotero/storage/PL4EYKBN/Elsken et al. - 2019 - Neural Architecture Search A Survey.pdf:application/pdf}
}



@incollection{mutualLearning,
address = {Cham},
title = {Mutual {Learning} to {Adapt} for {Joint} {Human} {Parsing} and {Pose} {Estimation}},
volume = {11209},
isbn = {978-3-030-01227-4 978-3-030-01228-1},
url = {http://link.springer.com/10.1007/978-3-030-01228-1_31},
abstract = {This paper presents a novel Mutual Learning to Adapt model (MuLA) for joint human parsing and pose estimation. It eﬀectively exploits mutual beneﬁts from both tasks and simultaneously boosts their performance. Diﬀerent from existing post-processing or multi-task learning based methods, MuLA predicts dynamic task-speciﬁc model parameters via recurrently leveraging guidance information from its parallel tasks. Thus MuLA can fast adapt parsing and pose models to provide more powerful representations by incorporating information from their counterparts, giving more robust and accurate results. MuLA is implemented with convolutional neural networks and end-to-end trainable. Comprehensive experiments on benchmarks LIP and extended PASCALPerson-Part demonstrate the eﬀectiveness of the proposed MuLA model with superior performance to well established baselines.},
language = {en},
urldate = {2020-02-26},
booktitle = {Computer {Vision} – {ECCV} 2018},
publisher = {Springer International Publishing},
author = {Nie, Xuecheng and Feng, Jiashi and Yan, Shuicheng},
editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
year = {2018},
doi = {10.1007/978-3-030-01228-1_31},
pages = {519--534},
file = {Nie et al. - 2018 - Mutual Learning to Adapt for Joint Human Parsing a.pdf:/home/konstaku/Zotero/storage/8QYIV5FG/Nie et al. - 2018 - Mutual Learning to Adapt for Joint Human Parsing a.pdf:application/pdf}
}



@inproceedings{multiLearningUncertainty,
address = {Salt Lake City, UT, USA},
title = {Multi-task {Learning} {Using} {Uncertainty} to {Weigh} {Losses} for {Scene} {Geometry} and {Semantics}},
isbn = {978-1-5386-6420-9},
url = {https://ieeexplore.ieee.org/document/8578879/},
doi = {10.1109/CVPR.2018.00781},
abstract = {Numerous deep learning applications beneﬁt from multitask learning with multiple regression and classiﬁcation objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task’s loss. Tuning these weights by hand is a difﬁcult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classiﬁcation and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.},
language = {en},
urldate = {2020-01-15},
booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
publisher = {IEEE},
author = {Cipolla, Roberto and Gal, Yarin and Kendall, Alex},
month = jun,
year = {2018},
pages = {7482--7491},
file = {Cipolla et al. - 2018 - Multi-task Learning Using Uncertainty to Weigh Los.pdf:/home/konstaku/Zotero/storage/JUFEPU8D/Cipolla et al. - 2018 - Multi-task Learning Using Uncertainty to Weigh Los.pdf:application/pdf}
}

@article{visualPerson,
title = {Visual {Person} {Understanding} through {Multi}-{Task} and {Multi}-{Dataset} {Learning}},
url = {http://arxiv.org/abs/1906.03019},
abstract = {We address the problem of learning a single model for person re-identiﬁcation, attribute classiﬁcation, body part segmentation, and pose estimation. With predictions for these tasks we gain a more holistic understanding of persons, which is valuable for many applications. This is a classical multi-task learning problem. However, no dataset exists that these tasks could be jointly learned from. Hence several datasets need to be combined during training, which in other contexts has often led to reduced performance in the past. We extensively evaluate how the different task and datasets inﬂuence each other and how diﬀerent degrees of parameter sharing between the tasks aﬀect performance. Our ﬁnal model matches or outperforms its single-task counterparts without creating signiﬁcant computational overhead, rendering it highly interesting for resource-constrained scenarios such as mobile robotics.},
language = {en},
urldate = {2020-01-29},
journal = {arXiv:1906.03019 [cs]},
author = {Pfeiffer, Kilian and Hermans, Alexander and Sárándi, István and Weber, Mark and Leibe, Bastian},
month = jun,
year = {2019},
note = {arXiv: 1906.03019},
keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
file = {Pfeiffer et al. - 2019 - Visual Person Understanding through Multi-Task and.pdf:/home/konstaku/Zotero/storage/XASEFZE4/Pfeiffer et al. - 2019 - Visual Person Understanding through Multi-Task and.pdf:application/pdf}
}

@article{multiDepth,
title = {{MultiDepth}: {Single}-{Image} {Depth} {Estimation} via {Multi}-{Task} {Regression} and {Classification}},
shorttitle = {{MultiDepth}},
url = {http://arxiv.org/abs/1907.11111},
abstract = {We introduce MultiDepth, a novel training strategy and convolutional neural network (CNN) architecture that allows approaching single-image depth estimation (SIDE) as a multi-task problem. SIDE is an important part of road scene understanding. It, thus, plays a vital role in advanced driver assistance systems and autonomous vehicles. Best results for the SIDE task so far have been achieved using deep CNNs. However, optimization of regression problems, such as estimating depth, is still a challenging task. For the related tasks of image classification and semantic segmentation, numerous CNN-based methods with robust training behavior have been proposed. Hence, in order to overcome the notorious instability and slow convergence of depth value regression during training, MultiDepth makes use of depth interval classification as an auxiliary task. The auxiliary task can be disabled at test-time to predict continuous depth values using the main regression branch more efficiently. We applied MultiDepth to road scenes and present results on the KITTI depth prediction dataset. In experiments, we were able to show that end-to-end multi-task learning with both, regression and classification, is able to considerably improve training and yield more accurate results.},
language = {en},
urldate = {2020-01-29},
journal = {arXiv:1907.11111 [cs]},
author = {Liebel, Lukas and Körner, Marco},
month = jul,
year = {2019},
note = {arXiv: 1907.11111},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Liebel and Körner - 2019 - MultiDepth Single-Image Depth Estimation via Mult.pdf:/home/konstaku/Zotero/storage/PWEGEBIU/Liebel and Körner - 2019 - MultiDepth Single-Image Depth Estimation via Mult.pdf:application/pdf}
}

@article{taskonomy,
title = {Taskonomy: {Disentangling} {Task} {Transfer} {Learning}},
abstract = {Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, in order to, for instance, seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity.},
language = {en},
author = {Zamir, Amir R and Sax, Alexander and Shen, William and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
pages = {12},
file = {Zamir et al. - Taskonomy Disentangling Task Transfer Learning.pdf:/home/konstaku/Zotero/storage/6T3QZWMI/Zamir et al. - Taskonomy Disentangling Task Transfer Learning.pdf:application/pdf}
}

@inproceedings{multiTaskAttention,
address = {Long Beach, CA, USA},
title = {End-{To}-{End} {Multi}-{Task} {Learning} {With} {Attention}},
isbn = {978-1-72813-293-8},
url = {https://ieeexplore.ieee.org/document/8954221/},
doi = {10.1109/CVPR.2019.00197},
abstract = {We propose a novel multi-task learning architecture, which allows learning of task-speciﬁc feature-level attention. Our design, the Multi-Task Attention Network (MTAN), consists of a single shared network containing a global feature pool, together with a soft-attention module for each task. These modules allow for learning of taskspeciﬁc features from the global features, whilst simultaneously allowing for features to be shared across different tasks. The architecture can be trained end-to-end and can be built upon any feed-forward neural network, is simple to implement, and is parameter efﬁcient. We evaluate our approach on a variety of datasets, across both image-toimage predictions and image classiﬁcation tasks. We show that our architecture is state-of-the-art in multi-task learning compared to existing methods, and is also less sensitive to various weighting schemes in the multi-task loss function. Code is available at https://github.com/ lorenmt/mtan.},
language = {en},
urldate = {2020-01-15},
booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
publisher = {IEEE},
author = {Liu, Shikun and Johns, Edward and Davison, Andrew J.},
month = jun,
year = {2019},
pages = {1871--1880},
file = {Liu et al. - 2019 - End-To-End Multi-Task Learning With Attention.pdf:/home/konstaku/Zotero/storage/6Z9IWAEJ/Liu et al. - 2019 - End-To-End Multi-Task Learning With Attention.pdf:application/pdf}
}

@article{crossStitch,
title = {Cross-stitch {Networks} for {Multi}-task {Learning}},
url = {http://arxiv.org/abs/1604.03539},
abstract = {Multi-task learning in Convolutional Networks has displayed remarkable success in the ﬁeld of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures speciﬁc to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multitask learning. Speciﬁcally, we propose a new sharing unit: “cross-stitch” unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-speciﬁc representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples.},
language = {en},
urldate = {2020-02-26},
journal = {arXiv:1604.03539 [cs]},
author = {Misra, Ishan and Shrivastava, Abhinav and Gupta, Abhinav and Hebert, Martial},
month = apr,
year = {2016},
note = {arXiv: 1604.03539},
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
file = {Misra et al. - 2016 - Cross-stitch Networks for Multi-task Learning.pdf:/home/konstaku/Zotero/storage/LFRIHM5A/Misra et al. - 2016 - Cross-stitch Networks for Multi-task Learning.pdf:application/pdf}
}


@article{learningToMultitask,
	title = {Learning to {Multitask}},
	abstract = {Multitask learning has shown promising performance in many applications and many multitask models have been proposed. In order to identify an effective multitask model for a given multitask problem, we propose a learning framework called Learning to MultiTask (L2MT). To achieve the goal, L2MT exploits historical multitask experience which is organized as a training set consisting of several tuples, each of which contains a multitask problem with multiple tasks, a multitask model, and the relative test error. Based on such training set, L2MT ﬁrst uses a proposed layerwise graph neural network to learn task embeddings for all the tasks in a multitask problem and then learns an estimation function to estimate the relative test error based on task embeddings and the representation of the multitask model based on a uniﬁed formulation. Given a new multitask problem, the estimation function is used to identify a suitable multitask model. Experiments on benchmark datasets show the effectiveness of the proposed L2MT framework.},
	language = {en},
	author = {Zhang, Yu and Wei, Ying and Yang, Qiang},
	pages = {12},
	file = {Zhang et al. - Learning to Multitask.pdf:/home/konstaku/Zotero/storage/CSZ75VG5/Zhang et al. - Learning to Multitask.pdf:application/pdf}
}

@inproceedings{usingUncertaintyToWeighLosses,
address = {Salt Lake City, UT, USA},
title = {Multi-task {Learning} {Using} {Uncertainty} to {Weigh} {Losses} for {Scene} {Geometry} and {Semantics}},
isbn = {978-1-5386-6420-9},
url = {https://ieeexplore.ieee.org/document/8578879/},
doi = {10.1109/CVPR.2018.00781},
abstract = {Numerous deep learning applications beneﬁt from multitask learning with multiple regression and classiﬁcation objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task’s loss. Tuning these weights by hand is a difﬁcult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classiﬁcation and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.},
language = {en},
urldate = {2020-01-15},
booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
publisher = {IEEE},
author = {Cipolla, Roberto and Gal, Yarin and Kendall, Alex},
month = jun,
year = {2018},
pages = {7482--7491},
file = {Cipolla et al. - 2018 - Multi-task Learning Using Uncertainty to Weigh Los.pdf:/home/konstaku/Zotero/storage/JUFEPU8D/Cipolla et al. - 2018 - Multi-task Learning Using Uncertainty to Weigh Los.pdf:application/pdf}
}

@article{lossWeighting,
	title = {A {Comparison} of {Loss} {Weighting} {Strategies} for {Multi} task {Learning} in {Deep} {Neural} {Networks}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2943604},
	abstract = {With the success of deep learning in a wide variety of areas, many deep multi-task learning (MTL) models have been proposed claiming improvements in performance obtained by sharing the learned structure across several related tasks. However, the dynamics of multi-task learning in deep neural networks is still not well understood at either the theoretical or experimental level. In particular, the usefulness of different task pairs is not known a priori. Practically, this means that properly combining the losses of different tasks becomes a critical issue in multi-task learning, as different methods may yield different results. In this paper, we benchmarked different multi-task learning approaches using shared trunk with task specific branches architecture across three different MTL datasets. For the first dataset, i.e. Multi-MNIST (Modified National Institute of Standards and Technology database), we thoroughly tested several weighting strategies, including simply adding task-specific cost functions together, dynamic weight average (DWA) and uncertainty weighting methods each with various amounts of training data per-task. We find that multi-task learning typically does not improve performance for a user-defined combination of tasks. Further experiments evaluated on diverse tasks and network architectures on various datasets suggested that multi-task learning requires careful selection of both task pairs and weighting strategies to equal or exceed the performance of single task learning.},
	journal = {IEEE Access},
	author = {Gong, Ting and Lee, Tyler and Stephenson, Cory and Renduchintala, Venkata and Padhy, Suchismita and Ndirango, Anthony and Keskin, Gokce and Elibol, Oguz H.},
	year = {2019},
	keywords = {Data models, Task analysis, Training, Deep learning, deep multitask learning models, deep neural networks, dynamic weight average, Dynamic weighting average, Estimation, learning (artificial intelligence), loss weighting strategies, multi-MNIST, multi-objective optimization, multi-task learning, neural nets, task pairs, task specific branches architecture, task-specific cost functions, Training data, Uncertainty, uncertainty weighting},
	pages = {141627--141632},
	file = {IEEE Xplore Abstract Record:/home/konstaku/Zotero/storage/XGGZ8FTG/8848395.html:text/html;IEEE Xplore Full Text PDF:/home/konstaku/Zotero/storage/R9J46AED/Gong et al. - 2019 - A Comparison of Loss Weighting Strategies for Mult.pdf:application/pdf}
}

@article{surveyOnMultiTask,
title = {A {Survey} on {Multi}-{Task} {Learning}},
url = {http://arxiv.org/abs/1707.08114},
abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL. First, we classify different MTL algorithms into several categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach, and decomposition approach, and then discuss the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, batch MTL models are difﬁcult to handle this situation and online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing are reviewed to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works.},
language = {en},
urldate = {2020-02-26},
journal = {arXiv:1707.08114 [cs]},
author = {Zhang, Yu and Yang, Qiang},
month = jul,
year = {2018},
note = {arXiv: 1707.08114},
keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
file = {Zhang and Yang - 2018 - A Survey on Multi-Task Learning.pdf:/home/konstaku/Zotero/storage/ZZDVVBMV/Zhang and Yang - 2018 - A Survey on Multi-Task Learning.pdf:application/pdf}
}

@article{origMultitask,
	title = {Multitask {Learning}},
	abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
	language = {en},
	author = {Caruana, Rich},
	year= {1997},
	pages = {35},
	file = {Caruana - Multitask Learning.pdf:/home/konstaku/Zotero/storage/5IZMB2MK/Caruana - Multitask Learning.pdf:application/pdf}
}


@article{whichTasks,
title = {Which {Tasks} {Should} {Be} {Learned} {Together} in {Multi}-task {Learning}?},
url = {http://arxiv.org/abs/1905.07553},
abstract = {Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using ‘multi-task learning’. This saves computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We systematically study task cooperation and competition and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.},
language = {en},
urldate = {2020-01-11},
journal = {arXiv:1905.07553 [cs]},
author = {Standley, Trevor and Zamir, Amir R. and Chen, Dawn and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
month = may,
year = {2019},
note = {arXiv: 1905.07553},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Standley et al. - 2019 - Which Tasks Should Be Learned Together in Multi-ta.pdf:/home/konstaku/Zotero/storage/8CCVP79Y/Standley et al. - 2019 - Which Tasks Should Be Learned Together in Multi-ta.pdf:application/pdf}
}


@inproceedings{selfDriving,
address = {Budapest, Hungary},
title = {Hierarchical {Multi}-{Task} {Learning} for {Healthy} {Drink} {Classification}},
isbn = {978-1-72811-985-4},
url = {https://ieeexplore.ieee.org/document/8851796/},
doi = {10.1109/IJCNN.2019.8851796},
abstract = {Recent advances in deep convolutional neural networks have enabled convenient diet tracking exploiting photos captured with smartphone cameras. However, most of the current diet tracking apps focus on recognizing solid foods while omitting drinks despite their negative impacts on our health when consumed without moderation. After an extensive analysis of drink images, we found that such an absence is due to the following challenges that conventional convolutional neural networks trained under the single-task learning framework cannot easily handle. First, drinks are amorphous. Second, visual cues of the drinks are often occluded and distorted by their container properties. Third, ingredients are inconspicuous because they often blend into the drink. In this work, we present a healthy drink classiﬁer trained under a hierarchical multi-task learning framework composed of a shared residual network with hierarchically shared convolutional layers between similar tasks and task-speciﬁc fully-connected layers. The proposed structure includes two main tasks, namely sugar level classiﬁcation and alcoholic drink recognition, and six auxiliary tasks, such as classiﬁcation and recognition of drink name, drink type, branding logo, container transparency, container shape, and container material. We also curated a drink dataset, Drink101, composed of 101 different drinks including 11,445 images overall. Our experimental results demonstrate improved classiﬁcation precision compared to single-task learning and baseline multi-task learning approaches.},
language = {en},
urldate = {2020-01-29},
booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
publisher = {IEEE},
author = {Park, Homin and Bharadhwaj, Homanga and Lim, Brian Y.},
month = jul,
year = {2019},
pages = {1--8},
file = {Park et al. - 2019 - Hierarchical Multi-Task Learning for Healthy Drink.pdf:/home/konstaku/Zotero/storage/P49PC6F6/Park et al. - 2019 - Hierarchical Multi-Task Learning for Healthy Drink.pdf:application/pdf}
}

@article{healthyDrink,
title = {End-to-end {Multi}-{Modal} {Multi}-{Task} {Vehicle} {Control} for {Self}-{Driving} {Cars} with {Visual} {Perception}},
url = {http://arxiv.org/abs/1801.06734},
abstract = {Convolutional Neural Networks (CNN) have been successfully applied to autonomous driving tasks, many in an endto-end manner. Previous end-to-end steering control methods take an image or an image sequence as the input and directly predict the steering angle with CNN. Although single task learning on steering angles has reported good performances, the steering angle alone is not sufﬁcient for vehicle control. In this work, we propose a multi-task learning framework to predict the steering angle and speed control simultaneously in an end-to-end manner. Since it is nontrivial to predict accurate speed values with only visual inputs, we ﬁrst propose a network to predict discrete speed commands and steering angles with image sequences. Moreover, we propose a multi-modal multi-task network to predict speed values and steering angles by taking previous feedback speeds and visual recordings as inputs. Experiments are conducted on the public Udacity dataset and a newly collected SAIC dataset. Results show that the proposed model predicts steering angles and speed values accurately. Furthermore, we improve the failure data synthesis methods to solve the problem of error accumulation in real road tests.},
language = {en},
urldate = {2020-01-29},
journal = {arXiv:1801.06734 [cs]},
author = {Yang, Zhengyuan and Zhang, Yixuan and Yu, Jerry and Cai, Junjie and Luo, Jiebo},
month = feb,
year = {2018},
note = {arXiv: 1801.06734},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Yang et al. - 2018 - End-to-end Multi-Modal Multi-Task Vehicle Control .pdf:/home/konstaku/Zotero/storage/HXUKUTXA/Yang et al. - 2018 - End-to-end Multi-Modal Multi-Task Vehicle Control .pdf:application/pdf}
}

@article{whatAndWhereToTransfer,
title = {Learning {What} and {Where} to {Transfer}},
url = {http://arxiv.org/abs/1905.05901},
abstract = {As the application of deep learning has expanded to real-world problems with insufﬁcient volume of training data, transfer learning recently has gained much attention as means of improving the performance in such small-data regime. However, when existing methods are applied between heterogeneous architectures and tasks, it becomes more important to manage their detailed conﬁgurations and often requires exhaustive tuning on them for the desired performance. To address the issue, we propose a novel transfer learning approach based on meta-learning that can automatically learn what knowledge to transfer from the source network to where in the target network. Given source and target networks, we propose an efﬁcient training scheme to learn meta-networks that decide (a) which pairs of layers between the source and target networks should be matched for knowledge transfer and (b) which features and how much knowledge from each feature should be transferred. We validate our meta-transfer approach against recent transfer learning methods on various datasets and network architectures, on which our automated scheme signiﬁcantly outperforms the prior baselines that ﬁnd “what and where to transfer” in a hand-crafted manner.},
language = {en},
urldate = {2019-12-09},
journal = {arXiv:1905.05901 [cs, stat]},
author = {Jang, Yunhun and Lee, Hankook and Hwang, Sung Ju and Shin, Jinwoo},
month = may,
year = {2019},
note = {arXiv: 1905.05901},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
file = {Jang et al. - 2019 - Learning What and Where to Transfer.pdf:/home/konstaku/Zotero/storage/264ZRMT9/Jang et al. - 2019 - Learning What and Where to Transfer.pdf:application/pdf}
}

@inproceedings{transferringMidLevelRepresentations,
address = {Columbus, OH, USA},
title = {Learning and {Transferring} {Mid}-level {Image} {Representations} {Using} {Convolutional} {Neural} {Networks}},
isbn = {978-1-4799-5118-5},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909618},
doi = {10.1109/CVPR.2014.222},
abstract = {Convolutional neural networks (CNN) have recently shown outstanding image classiﬁcation performance in the largescale visual recognition challenge (ILSVRC2012). The success of CNNs is attributed to their ability to learn rich midlevel image representations as opposed to hand-designed low-level features used in other image classiﬁcation methods. Learning CNNs, however, amounts to estimating millions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be efﬁciently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred representation leads to signiﬁcantly improved results for object and action classiﬁcation, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.},
language = {en},
urldate = {2020-01-29},
booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
publisher = {IEEE},
author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
month = jun,
year = {2014},
pages = {1717--1724},
file = {Oquab et al. - 2014 - Learning and Transferring Mid-level Image Represen.pdf:/home/konstaku/Zotero/storage/3WWZQLSF/Oquab et al. - 2014 - Learning and Transferring Mid-level Image Represen.pdf:application/pdf}
}

@inproceedings{multiTaskWeather,
address = {Mountain View, California, USA},
title = {A {Multi}-{Task} {Framework} for {Weather} {Recognition}},
isbn = {978-1-4503-4906-2},
url = {http://dl.acm.org/citation.cfm?doid=3123266.3123382},
doi = {10.1145/3123266.3123382},
abstract = {Weather recognition is important in practice, while this task has not been thoroughly explored so far. The current trend of dealing with this task is treating it as a single classi cation problem, i.e., determining whether a given image belongs to a certain weather category or not. However, weather recognition di ers signi cantly from traditional image classi cation, since several weather features may appear simultaneously. In this case, a simple classi cation result is insu cient to describe the weather condition. To address this issue, we propose to provide auxiliary weather related information for comprehensive weather description. Speci cally, semantic segmentation of weather-cues, such as blue sky and white clouds, is exploited as an auxiliary task in this paper. Moreover, a convolutional neural network (CNN) based multi-task framework is developed which aims to concurrently tackle weather category classi cation task and weather-cues segmentation task. Due to the intrinsic relationships between these two tasks, exploring auxiliary semantic segmentation of weather-cues can also help to learn discriminative features for the classi cation task, and thus obtain superior accuracy. To verify the e ectiveness of the proposed approach, extra segmentation masks of weather-cues are generated manually on an existing weather image dataset. Experimental results have demonstrated the superior performance of our approach. The enhanced dataset, source codes and pre-trained models are available at https://github.com/wzgwzg/Multitask\_Weather.},
language = {en},
urldate = {2019-12-06},
booktitle = {Proceedings of the 2017 {ACM} on {Multimedia} {Conference} - {MM} '17},
publisher = {ACM Press},
author = {Li, Xuelong and Wang, Zhigang and Lu, Xiaoqiang},
year = {2017},
pages = {1318--1326},
file = {Li et al. - 2017 - A Multi-Task Framework for Weather Recognition.pdf:/home/konstaku/Zotero/storage/SYNZBKS3/Li et al. - 2017 - A Multi-Task Framework for Weather Recognition.pdf:application/pdf}
}

@article{weatherNet,
title = {{WeatherNet}: {Recognising} {Weather} and {Visual} {Conditions} from {Street}-{Level} {Images} {Using} {Deep} {Residual} {Learning}},
volume = {8},
issn = {2220-9964},
shorttitle = {{WeatherNet}},
url = {https://www.mdpi.com/2220-9964/8/12/549},
doi = {10.3390/ijgi8120549},
abstract = {Extracting information related to weather and visual conditions at a given time and space is indispensable for scene awareness, which strongly impacts our behaviours, from simply walking in a city to riding a bike, driving a car, or autonomous driveassistance. Despite the significance of this subject, it is still not been fully addressed by the machine intelligence relying on deep learning and computer vision to detect the multi-labels of weather and visual conditions with a unified method that can be easily used for practice. What has been achieved to-date is rather sectorial models that address limited number of labels that do not cover the wide spectrum of weather and visual conditions. Nonetheless, weather and visual conditions are often addressed individually. In this paper, we introduce a novel framework to automatically extract this information from street-level images relying on deep learning and computer vision using a unified method without any pre-defined constraints in the processed images. A pipeline of four deep Convolutional Neural Network (CNN) models, so-called the WeatherNet, is trained, relying on residual learning using ResNet50 architecture, to extract various weather and visual conditions such as Dawn/dusk, day and night for time detection, and glare for lighting conditions, and clear, rainy, snowy, and foggy for weather conditions. The WeatherNet shows strong performance in extracting this information from user-defined images or video streams that can be used not limited to: autonomous vehicles and drive-assistance systems, tracking behaviours, safety-related research, or even for better understanding cities through images for policy-makers.},
language = {en},
number = {12},
urldate = {2019-12-03},
journal = {ISPRS International Journal of Geo-Information},
author = {Ibrahim, Mohamed R. and Haworth, James and Cheng, Tao},
month = nov,
year = {2019},
pages = {549},
file = {Ibrahim et al. - 2019 - WeatherNet Recognising Weather and Visual Conditi.pdf:/home/konstaku/Zotero/storage/V3IZJE9G/Ibrahim et al. - 2019 - WeatherNet Recognising Weather and Visual Conditi.pdf:application/pdf}
}
@article{biologicalMultitask,
title = {Deep {Model} {Based} {Transfer} and {Multi}-{Task} {Learning} for {Biological} {Image} {Analysis}},
issn = {2372-2096},
doi = {10.1109/TBDATA.2016.2573280},
abstract = {A central theme in learning from image data is to develop appropriate representations for the specific task at hand. Thus, a practical challenge is to determine what features are appropriate for specific tasks. For example, in the study of gene expression patterns in Drosophila, texture features were particularly effective for determining the developmental stages from in situ hybridization images. Such image representation is however not suitable for controlled vocabulary term annotation. Here, we developed feature extraction methods to generate hierarchical representations for ISH images. Our approach is based on the deep convolutional neural networks that can act on image pixels directly. To make the extracted features generic, the models were trained using a natural image set with millions of labeled examples. These models were transferred to the ISH image domain. To account for the differences between the source and target domains, we proposed a partial transfer learning scheme in which only part of the source model is transferred. We employed multi-task learning method to fine-tune the pre-trained models with labeled ISH images. Results showed that feature representations computed by deep models based on transfer and multi-task learning significantly outperformed other methods for annotating gene expression patterns at different stage ranges.},
journal = {IEEE Transactions on Big Data},
author = {Zhang, Wenlu and Li, Rongjian and Zeng, Tao and Sun, Qian and Kumar, Sudhir and Ye, Jieping and Ji, Shuiwang},
year = {2016},
note = {Conference Name: IEEE Transactions on Big Data},
keywords = {bioinformatics, Biological system modeling, Computational modeling, Data models, Deep learning, Feature extraction, Gene expression, image analysis, multi-task learning, Training, transfer learning},
pages = {1--1},
file = {IEEE Xplore Abstract Record:/home/konstaku/Zotero/storage/64X4BQHS/7480825.html:text/html}
}

@article{uberNet,
title = {{UberNet}: {Training} a `{Universal}' {Convolutional} {Neural} {Network} for {Low}-, {Mid}-, and {High}-{Level} {Vision} using {Diverse} {Datasets} and {Limited} {Memory}},
shorttitle = {{UberNet}},
url = {http://arxiv.org/abs/1609.02132},
abstract = {In this work we introduce a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a uniﬁed architecture that is trained end-to-end. Such a universal network can act like a ‘swiss knife’ for vision tasks; we call this architecture an UberNet to indicate its overarching nature.},
language = {en},
urldate = {2020-03-11},
journal = {arXiv:1609.02132 [cs]},
author = {Kokkinos, Iasonas},
month = sep,
year = {2016},
note = {arXiv: 1609.02132},
keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
file = {Kokkinos - 2016 - UberNet Training a `Universal' Convolutional Neur.pdf:/home/konstaku/Zotero/storage/4A8L9JV7/Kokkinos - 2016 - UberNet Training a `Universal' Convolutional Neur.pdf:application/pdf}
}

@article{ruderOverview,
title = {An {Overview} of {Multi}-{Task} {Learning} in {Deep} {Neural} {Networks}},
url = {http://arxiv.org/abs/1706.05098},
abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
language = {en},
urldate = {2020-01-15},
journal = {arXiv:1706.05098 [cs, stat]},
author = {Ruder, Sebastian},
month = jun,
year = {2017},
note = {arXiv: 1706.05098},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
file = {Ruder - 2017 - An Overview of Multi-Task Learning in Deep Neural .pdf:/home/konstaku/Zotero/storage/P935YLYB/Ruder - 2017 - An Overview of Multi-Task Learning in Deep Neural .pdf:application/pdf}
}

@incollection{multiPoseNet,
address = {Cham},
title = {{MultiPoseNet}: {Fast} {Multi}-{Person} {Pose} {Estimation} {Using} {Pose} {Residual} {Network}},
volume = {11215},
isbn = {978-3-030-01251-9 978-3-030-01252-6},
shorttitle = {{MultiPoseNet}},
url = {http://link.springer.com/10.1007/978-3-030-01252-6_26},
abstract = {In this paper, we present MultiPoseNet, a novel bottom-up multi-person pose estimation architecture that combines a multi-task model with a novel assignment method. MultiPoseNet can jointly handle person detection, person segmentation and pose estimation problems. The novel assignment method is implemented by the Pose Residual Network (PRN) which receives keypoint and person detections, and produces accurate poses by assigning keypoints to person instances. On the COCO keypoints dataset, our pose estimation method outperforms all previous bottom-up methods both in accuracy (+4-point mAP over previous best result) and speed; it also performs on par with the best top-down methods while being at least 4x faster. Our method is the fastest real time system with ∼23 frames/sec.},
language = {en},
urldate = {2020-03-12},
booktitle = {Computer {Vision} – {ECCV} 2018},
publisher = {Springer International Publishing},
author = {Kocabas, Muhammed and Karagoz, Salih and Akbas, Emre},
editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
year = {2018},
doi = {10.1007/978-3-030-01252-6_26},
note = {Series Title: Lecture Notes in Computer Science},
pages = {437--453},
file = {Kocabas et al. - 2018 - MultiPoseNet Fast Multi-Person Pose Estimation Us.pdf:/home/konstaku/Zotero/storage/EBABUMHZ/Kocabas et al. - 2018 - MultiPoseNet Fast Multi-Person Pose Estimation Us.pdf:application/pdf}
}




@article{attention,
title = {Attention {Is} {All} {You} {Need}},
url = {http://arxiv.org/abs/1706.03762},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
language = {en},
urldate = {2020-03-31},
journal = {arXiv:1706.03762 [cs]},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
month = dec,
year = {2017},
note = {arXiv: 1706.03762},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
file = {Vaswani et al. - 2017 - Attention Is All You Need.pdf:/home/konstaku/Zotero/storage/PBH7RGMN/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf}
}

@article{cnn-rnn,

title = {A {CNN}-{RNN} {Architecture} for {Multi}-{Label} {Weather} {Recognition}},
url = {http://arxiv.org/abs/1904.10709},
abstract = {Weather Recognition plays an important role in our daily lives and many computer vision applications. However, recognizing the weather conditions from a single image remains challenging and has not been studied thoroughly. Generally, most previous works treat weather recognition as a single-label classiﬁcation task, namely, determining whether an image belongs to a speciﬁc weather class or not. This treatment is not always appropriate, since more than one weather conditions may appear simultaneously in a single image. To address this problem, we make the ﬁrst attempt to view weather recognition as a multi-label classiﬁcation task, i.e., assigning an image more than one labels according to the displayed weather conditions. Speciﬁcally, a CNN-RNN based multi-label classiﬁcation approach is proposed in this paper. The convolutional neural network (CNN) is extended with a channel-wise attention model to extract the most correlated visual features. The Recurrent Neural Network (RNN) further processes the features and excavates the dependencies among weather classes. Finally, the weather labels are predicted step by step. Besides, we construct two datasets for the weather recognition task and explore the relationships among different weather conditions. Experimental results demonstrate the superiority and effectiveness of the proposed approach. The new constructed datasets will be available at https: //github.com/wzgwzg/Multi-Label-Weather-Recognition.},
language = {en},
urldate = {2019-12-03},
journal = {arXiv:1904.10709 [cs]},
author = {Zhao, Bin and Li, Xuelong and Lu, Xiaoqiang and Wang, Zhigang},
month = apr,
year = {2019},
note = {arXiv: 1904.10709},
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
file = {Zhao et al. - 2019 - A CNN-RNN Architecture for Multi-Label Weather Rec.pdf:/home/konstaku/Zotero/storage/39R5XSWH/Zhao et al. - 2019 - A CNN-RNN Architecture for Multi-Label Weather Rec.pdf:application/pdf}
}

@misc{nvidiaRTX,
title = {Introducing {NVIDIA} {GeForce} {RTX} 2070 {Graphics} {Card}},
url = {https://www.nvidia.com/fi-fi/geforce/graphics-cards/rtx-2070/},
abstract = {Nauti reaaliaikaisesta säteenseurannasta, tekoälystä ja jopa kuusinkertaisesta suorituskyvystä edellisen sukupolven näytönohjaimiin verrattuna.{\textless}br/{\textgreater}},
language = {fi-fi},
urldate = {2020-03-25},
journal = {NVIDIA},
note = {Library Catalog: www.nvidia.com},
file = {Snapshot:/home/konstaku/Zotero/storage/TRZXWNM3/rtx-2070.html:text/html}
}

@article{pytorch,
title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
shorttitle = {{PyTorch}},
url = {http://arxiv.org/abs/1912.01703},
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientiﬁc computing libraries, while remaining efﬁcient and supporting hardware accelerators such as GPUs.},
language = {en},
urldate = {2020-03-25},
journal = {arXiv:1912.01703 [cs, stat]},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
month = dec,
year = {2019},
note = {arXiv: 1912.01703},
keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
file = {Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:/home/konstaku/Zotero/storage/MJZR638D/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf}
}

@misc{Apex,
	title = {{NVIDIA}/apex},
	copyright = {BSD-3-Clause},
	url = {https://github.com/NVIDIA/apex},
	abstract = {A PyTorch Extension:  Tools for easy mixed precision and distributed training in Pytorch},
	urldate = {2020-03-25},
	publisher = {NVIDIA Corporation},
	month = mar,
	year = {2020},
	note = {original-date: 2018-04-23T16:28:52Z}
}

@article{mixedTraining,
title = {Mixed {Precision} {Training}},
url = {http://arxiv.org/abs/1710.03740},
abstract = {Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision ﬂoating point numbers, without losing model accuracy or having to modify hyperparameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE halfprecision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to halfprecision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.},
language = {en},
urldate = {2020-03-25},
journal = {arXiv:1710.03740 [cs, stat]},
author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
month = feb,
year = {2018},
note = {arXiv: 1710.03740},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
file = {Micikevicius et al. - 2018 - Mixed Precision Training.pdf:/home/konstaku/Zotero/storage/7U2RPXCE/Micikevicius et al. - 2018 - Mixed Precision Training.pdf:application/pdf}
}


@misc{docker,
	title = {Empowering {App} {Development} for {Developers} {\textbar} {Docker}},
	url = {https://www.docker.com/},
	abstract = {Learn how Docker helps developers bring their ideas to life by conquering the complexity of app development.},
	language = {en},
	urldate = {2020-03-25},
	note = {Library Catalog: www.docker.com},
	file = {Snapshot:/home/konstaku/Zotero/storage/66KSUDIL/www.docker.com.html:text/html}
}
@ONLINE {beansdata,
	author="Makerere AI Lab",
	title="Bean disease dataset",
	month="January",
	year="2020",
	url="https://github.com/AI-Lab-Makerere/ibean/"
}

@ONLINE {tfflowers,
	author = "The TensorFlow Team",
	title = "Flowers",
	month = "jan",
	year = "2019",
	url = "http://download.tensorflow.org/example_images/flower_photos.tgz" 
	}

@article{citrusdata,
	title={A citrus fruits and leaves dataset for detection and classification of
			citrus diseases through machine learning},
	author={Rauf, Hafiz Tayyab and Saleem, Basharat Ali and Lali, M Ikram Ullah
			and Khan, Muhammad Attique and Sharif, Muhammad and Bukhari, Syed Ahmad Chan},
	journal={Data in brief},
	volume={26},
	pages={104340},
	year={2019},
	publisher={Elsevier}
}

@article{cycliclr,
title = {Cyclical {Learning} {Rates} for {Training} {Neural} {Networks}},
url = {http://arxiv.org/abs/1506.01186},
abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally ﬁnd the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of ﬁxed values achieves improved classiﬁcation accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate “reasonable bounds” – linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
language = {en},
urldate = {2020-03-25},
journal = {arXiv:1506.01186 [cs]},
author = {Smith, Leslie N.},
month = apr,
year = {2017},
note = {arXiv: 1506.01186},
keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
file = {Smith - 2017 - Cyclical Learning Rates for Training Neural Networ.pdf:/home/konstaku/Zotero/storage/I4FER6E2/Smith - 2017 - Cyclical Learning Rates for Training Neural Networ.pdf:application/pdf}
}

@article{attention,
title = {Attention {Is} {All} {You} {Need}},
url = {http://arxiv.org/abs/1706.03762},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
language = {en},
urldate = {2020-03-31},
journal = {arXiv:1706.03762 [cs]},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
month = dec,
year = {2017},
note = {arXiv: 1706.03762},
keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
file = {Vaswani et al. - 2017 - Attention Is All You Need.pdf:/home/konstaku/Zotero/storage/PBH7RGMN/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf}
}

@article{cnn-rnn,

title = {A {CNN}-{RNN} {Architecture} for {Multi}-{Label} {Weather} {Recognition}},
url = {http://arxiv.org/abs/1904.10709},
abstract = {Weather Recognition plays an important role in our daily lives and many computer vision applications. However, recognizing the weather conditions from a single image remains challenging and has not been studied thoroughly. Generally, most previous works treat weather recognition as a single-label classiﬁcation task, namely, determining whether an image belongs to a speciﬁc weather class or not. This treatment is not always appropriate, since more than one weather conditions may appear simultaneously in a single image. To address this problem, we make the ﬁrst attempt to view weather recognition as a multi-label classiﬁcation task, i.e., assigning an image more than one labels according to the displayed weather conditions. Speciﬁcally, a CNN-RNN based multi-label classiﬁcation approach is proposed in this paper. The convolutional neural network (CNN) is extended with a channel-wise attention model to extract the most correlated visual features. The Recurrent Neural Network (RNN) further processes the features and excavates the dependencies among weather classes. Finally, the weather labels are predicted step by step. Besides, we construct two datasets for the weather recognition task and explore the relationships among different weather conditions. Experimental results demonstrate the superiority and effectiveness of the proposed approach. The new constructed datasets will be available at https: //github.com/wzgwzg/Multi-Label-Weather-Recognition.},
language = {en},
urldate = {2019-12-03},
journal = {arXiv:1904.10709 [cs]},
author = {Zhao, Bin and Li, Xuelong and Lu, Xiaoqiang and Wang, Zhigang},
month = apr,
year = {2019},
note = {arXiv: 1904.10709},
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
file = {Zhao et al. - 2019 - A CNN-RNN Architecture for Multi-Label Weather Rec.pdf:/home/konstaku/Zotero/storage/39R5XSWH/Zhao et al. - 2019 - A CNN-RNN Architecture for Multi-Label Weather Rec.pdf:application/pdf}
}

@InProceedings{flowers102,
  author       = "Maria-Elena Nilsback and Andrew Zisserman",
  title        = "Automated Flower Classification over a Large Number of Classes",
  booktitle    = "Indian Conference on Computer Vision, Graphics and Image Processing",
  month        = "Dec",
  year         = "2008",
}
