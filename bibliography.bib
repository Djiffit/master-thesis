@article{einstein,
author =       "Albert Einstein",
title =        "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
[{On} the electrodynamics of moving bodies]",
journal =      "Annalen der Physik",
volume =       "322",
number =       "10",
pages =        "891--921",
year =         "1905",
DOI =          "http://dx.doi.org/10.1002/andp.19053221004"
}

@article{guerra_weather_2018,
title = {Weather {Classification}: {A} new multi-class dataset, data augmentation approach and comprehensive evaluations of {Convolutional} {Neural} {Networks}},
shorttitle = {Weather {Classification}},
url = {http://arxiv.org/abs/1808.00588},
abstract = {Weather conditions often disrupt the proper functioning of transportation systems. Present systems either deploy an array of sensors or use an in-vehicle camera to predict weather conditions. These solutions have resulted in incremental cost and limited scope. To ensure smooth operation of all transportation services in all-weather conditions, a reliable detection system is necessary to classify weather in wild. The challenges involved in solving this problem is that weather conditions are diverse in nature and there is an absence of discriminate features among various weather conditions. The existing works to solve this problem have been scene speciﬁc and have targeted classiﬁcation of two categories of weather. In this paper, we have created a new open source dataset consisting of images depicting three classes of weather i.e rain, snow and fog called RFS Dataset. A novel algorithm has also been proposed which has used super pixel delimiting masks as a form of data augmentation, leading to reasonable results with respect to ten Convolutional Neural Network architectures.},
language = {en},
urldate = {2019-12-03},
journal = {arXiv:1808.00588 [cs]},
author = {Guerra, Jose Carlos Villarreal and Khanam, Zeba and Ehsan, Shoaib and Stolkin, Rustam and McDonald-Maier, Klaus},
month = aug,
year = {2018},
note = {arXiv: 1808.00588},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Guerra et al. - 2018 - Weather Classification A new multi-class dataset,.pdf:/Users/kokutvon/Zotero/storage/69INUZPT/Guerra et al. - 2018 - Weather Classification A new multi-class dataset,.pdf:application/pdf}
}

@article{ibrahim_weathernet:_2019,
title = {{WeatherNet}: {Recognising} {Weather} and {Visual} {Conditions} from {Street}-{Level} {Images} {Using} {Deep} {Residual} {Learning}},
volume = {8},
issn = {2220-9964},
shorttitle = {{WeatherNet}},
url = {https://www.mdpi.com/2220-9964/8/12/549},
doi = {10.3390/ijgi8120549},
abstract = {Extracting information related to weather and visual conditions at a given time and space is indispensable for scene awareness, which strongly impacts our behaviours, from simply walking in a city to riding a bike, driving a car, or autonomous driveassistance. Despite the significance of this subject, it is still not been fully addressed by the machine intelligence relying on deep learning and computer vision to detect the multi-labels of weather and visual conditions with a unified method that can be easily used for practice. What has been achieved to-date is rather sectorial models that address limited number of labels that do not cover the wide spectrum of weather and visual conditions. Nonetheless, weather and visual conditions are often addressed individually. In this paper, we introduce a novel framework to automatically extract this information from street-level images relying on deep learning and computer vision using a unified method without any pre-defined constraints in the processed images. A pipeline of four deep Convolutional Neural Network (CNN) models, so-called the WeatherNet, is trained, relying on residual learning using ResNet50 architecture, to extract various weather and visual conditions such as Dawn/dusk, day and night for time detection, and glare for lighting conditions, and clear, rainy, snowy, and foggy for weather conditions. The WeatherNet shows strong performance in extracting this information from user-defined images or video streams that can be used not limited to: autonomous vehicles and drive-assistance systems, tracking behaviours, safety-related research, or even for better understanding cities through images for policy-makers.},
language = {en},
number = {12},
urldate = {2019-12-03},
journal = {IJGI},
author = {Ibrahim, Mohamed R. and Haworth, James and Cheng, Tao},
month = nov,
year = {2019},
pages = {549},
file = {Ibrahim et al. - 2019 - WeatherNet Recognising Weather and Visual Conditi.pdf:/Users/kokutvon/Zotero/storage/V3IZJE9G/Ibrahim et al. - 2019 - WeatherNet Recognising Weather and Visual Conditi.pdf:application/pdf}
}

@article{pan_winter_nodate,
title = {Winter {Road} {Surface} {Condition} {Recognition} {Using} a {Pre}-trained {Deep} {Convolutional} {Neural} {Network}},
language = {en},
author = {Pan, Guangyuan and Fu, Liping and Yu, Ruifan and Muresan, Matthew},
pages = {13},
file = {Pan et al. - Winter Road Surface Condition Recognition Using a .pdf:/Users/kokutvon/Zotero/storage/AQWPYSCB/Pan et al. - Winter Road Surface Condition Recognition Using a .pdf:application/pdf}
}
@book{latexcompanion,
	author    = "Michel Goossens and Frank Mittelbach and Alexander Samarin",
	title     = "The \LaTeX\ Companion",
	year      = "1993",
	publisher = "Addison-Wesley",
	address   = "Reading, Massachusetts"
}

@book{knuth99,
	author    = "Donald E. Knuth",
	title     = "Digital Typography",
	year      = "1999",
	publisher = "The Center for the Study of Language and Information",
	series    = "CLSI Lecture Notes (78)"
}

@misc{knuthwebsite,
	author    = "Donald Knuth",
	title     = "Knuth: Computers and Typesetting",
	url       = "http://www-cs-faculty.stanford.edu/~knuth/abcde.html"
}

@TECHREPORT{erkio01,
	AUTHOR  = "Erkiö, H. and Mäkelä, M. and Nykänen, M. and Verkamo, I.",
	TITLE   = "Opinnäytetyän ulkoasun malli.",
	INSTITUTION = "Tietojenkäsittelyopin laitos",
	YEAR        = 2001,
	TYPE        = "Tieteellisen kirjoittamisen kurssiin
	liittyvä julkaisematon moniste",
	ADDRESS     = "Helsinki"
}



@ARTICLE{crowdenetal79,
AUTHOR  = "Crowder, H. and Dembo, R.S. and Mulvey, J.M.",
TITLE   = "On reporting computational experiments
with mathematical software",
JOURNAL = "{ACM} Transactions on Mathematical Software",
YEAR    = 1979,
VOLUME  = 5,
NUMBER  = 2,
PAGES   = "193--203"
}

@INPROCEEDINGS{dantowsley90,
AUTHOR       = "Dan, A. and Towsley, D.",
TITLE        = "An approximate analysis of the {LRU} and {FIFO}
buffer replacement schemes",
BOOKTITLE    = "Proc. {ACM} Conf.\ Measurement and
Modeling of Computer Systems",
YEAR         = 1990,
PAGES        = "143--152",
ADDRESS      = "Boulder, Colorado, {USA}",
MONTH        = may
}

@TECHREPORT{erkio94,
	AUTHOR      = "Erkiö, H.",
	TITLE   = "Opinnäytetyän ulkoasun malli.",
	INSTITUTION = "Tietojenkäsittelyopin laitos",
	YEAR        = 1994,
	TYPE        = "Tieteellisen kirjoittamisen kurssiin
	liittyvä julkaisematon moniste",
	ADDRESS     = "Helsinki"
}

@TECHREPORT{erkiomakela96,
	AUTHOR      = "Erkiö, H. and Mäkelä, M.",
	TITLE       = "Opinnäytetyän ulkoasun malli",
	INSTITUTION = "Tietojenkäsittelytieteen laitos",
	YEAR        = 1996,
	TYPE        = "Tieteellisen kirjoittamisen kurssiin
	liittyvä julkaisematon moniste",
	ADDRESS     = "Helsinki"
}

@BOOK{fogelbergetal89,
	AUTHOR    = "Fogelberg, P. and Herranen, M. and Sinikara, K.",
	TITLE     = "Tuumasta toimeen, tutkielman tekijän opas",
	PUBLISHER = "Yliopistopaino",
	YEAR      = 1989,
	ADDRESS   = "Helsinki"
}

@INCOLLECTION{gannonetal89,
	AUTHOR    = "Gannon, D. and others",
	TITLE     = "Programming environments for parallel algorithms",
	BOOKTITLE = "Parallel and Distributed Algorithms",
	YEAR      = 1989,
	EDITOR    = "M. Cosnard and others",
	publisher    = "North-Holland",
	PAGES     = "101--108"
}

@BOOK{grimm87,
	AUTHOR    = "Grimm, S. S.",
	TITLE     = "How to write computer documentation for users",
	PUBLISHER = "Van Nostrand Reinhold Co.",
	YEAR      = 1987,
	ADDRESS   = "New York"
}

@BOOK{harkinsplung82,
	EDITOR    = "Harkins, C. and Plung, D. L.",
	TITLE     = "A guide for writing better technical papers",
	PUBLISHER = "IEEE Press",
	YEAR      = 1982}

@TECHREPORT{julkohj81,
AUTHOR      = "{Helsingin yliopisto}",
TITLE       = "Julkaisusarjoja ja opinnäytteiden tiivistelmiä
koskevat ohjeet ja suositukset",
YEAR        = 1981,
TYPE        = "Helsingin yliopiston kirjastolaitoksen julkaisu {A 3}",
ADDRESS     = "Helsinki"
}

@WWW-MISC{kilpelainen00,
AUTHOR       = "Kilpeläinen, P.",
TITLE        = "{WWW}-lähteisiin viittaaminen tutkielmatekstissä",
URL          = "http://www.cs.helsinki.fi/u/kilpelai/TiKi/wwwlahteet.html",
YEAR         =  2000,
TYPE        = "Tieteellisen kirjoittamisen kurssiin
liittyvä julkaisematon moniste",
VALID        =  "19.1.2000"
}

@ARTICLE{smith78a,
AUTHOR  = "Smith, A. J.",
TITLE   = "Sequentiality and prefetching in database systems",
JOURNAL = "{ACM} Transactions on Database Systems",
YEAR    = 1978,
VOLUME  = 3,
NUMBER  = 3,
PAGES   = "223--247"
}

@ARTICLE{smith78b,
	AUTHOR  = "Smith, A. J.",
	TITLE   = "Sequential program prefetching in memory hierarchies",
	JOURNAL = "Computer",
	YEAR    = 1978,
	VOLUME  = 11,
	NUMBER  = 11,
	PAGES   = "7--21"
}

@TECHREPORT{verkamo92,
	AUTHOR      = "Verkamo, A. I.",
	TITLE       = "Opinnäytetyän ulkoasun malli",
	INSTITUTION = "Tietojenkäsittelyopin laitos",
	YEAR        = 1992,
	TYPE        = "Tieteellisen kirjoittamisen kurssiin
	liittyvä julkaisematon moniste",
	ADDRESS     = "Helsinki"
}

@ARTICLE{abiteboul,

AUTHOR  = "Abiteboul, S. and Quass, D. and McHugh, J. and
Widom, J. and Wiener, J.L.",
TITLE   = "The Lorel query language for semistructured data",
JOURNAL = "International Journal on Digital Libraries",
YEAR    = 1997,
VOLUME  = 1,
NUMBER  = 1,
PAGES   = "68-88",
NOTE    = "[
\url{http://link.springer.de/
	link/service/journals/00799/
	bibs/7001001/70010068.htm},
18.1.2000]"
}


@misc{bray,
AUTHOR = "Bray, T. and Paoli, J. and Sperberg-McQueen, C.M.",
TITLE  = "{Extensible Markup Language {XML} 1.0. W3C Recommendation 10-February-1998}",
howpublished    = "[\url{http://www.w3.org/TR/1998/REC-xml-19980210}, 02.11.2016]"
}

@ARTICLE{dietinger,

AUTHOR  = "Dietinger, T. and others",
TITLE   = "{Dynamic Background Libraries - New Developments in
Distance Education Using {HIKS} {Hierarchical Interactive
		Knowledge System}}",
JOURNAL = "Journal of Universal Computer Science",
YEAR    = 1999,
VOLUME  = 5,
NUMBER  = 1,
NOTE    = "[\url{http://www.iicm.edu/
	jucs_5_1/dynamic_background_libraries_new},
18.1.2000]"
}

@article{alexNet,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {00010782},
	url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2019-12-06},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/home/konstaku/Zotero/storage/45HSR2RH/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf}
}

@article{imagenet,
title = {{ImageNet}: {A} {Large}-{Scale} {Hierarchical} {Image} {Database}},
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
language = {en},
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
pages = {8},
year = {2010},
file = {Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf:/home/konstaku/Zotero/storage/F5WWKAJK/Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf:application/pdf}
}


@misc{ILSVRC,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Competition} ({ILSVRC})},
	url = {http://www.image-net.org/challenges/LSVRC/},
	urldate = {2020-01-26},
	file = {ImageNet Large Scale Visual Recognition Competition (ILSVRC):/home/konstaku/Zotero/storage/2IGV2WF3/LSVRC.html:text/html}
}

@article{resNet,
title = {Deep {Residual} {Learning} for {Image} {Recognition}},
url = {http://arxiv.org/abs/1512.03385},
abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
language = {en},
urldate = {2019-12-06},
journal = {arXiv:1512.03385 [cs]},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
month = dec,
year = {2015},
note = {arXiv: 1512.03385},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
annote = {Comment: Tech report},
file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/home/konstaku/Zotero/storage/CMC3Y7YX/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@article{imageNet_summary,
title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
url = {http://arxiv.org/abs/1409.0575},
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classiﬁcation and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than ﬁfty institutions.},
language = {en},
urldate = {2020-01-26},
journal = {arXiv:1409.0575 [cs]},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
month = jan,
year = {2015},
note = {arXiv: 1409.0575},
keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.8, I.5.2},
file = {Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:/home/konstaku/Zotero/storage/7U9W6FSF/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:application/pdf}
}

@article{objectNet,
title = {{ObjectNet}: {A} large-scale bias-controlled dataset for pushing the limits of object recognition models},
abstract = {We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientiﬁc experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be ﬁne-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45\% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to ﬁne-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet – objects are largely centered and unoccluded – and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.},
language = {en},
year = {2019},
author = {Barbu, Andrei and Mayo, David and Alverio, Julian and Luo, William and Wang, Christopher and Gutfreund, Dan and Tenenbaum, Josh and Katz, Boris},
pages = {11},
file = {Barbu et al. - ObjectNet A large-scale bias-controlled dataset f.pdf:/home/konstaku/Zotero/storage/QIHM7LV9/Barbu et al. - ObjectNet A large-scale bias-controlled dataset f.pdf:application/pdf}
}
@article{efficientNet,
title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
shorttitle = {{EfficientNet}},
url = {http://arxiv.org/abs/1905.11946},
abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a ﬁxed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefﬁcient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.},
language = {en},
urldate = {2019-12-09},
journal = {arXiv:1905.11946 [cs, stat]},
author = {Tan, Mingxing and Le, Quoc V.},
month = nov,
year = {2019},
note = {arXiv: 1905.11946},
keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
file = {Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf:/home/konstaku/Zotero/storage/IA4SQF8Z/Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf}
}

@article{betterTransfer,
title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
url = {http://arxiv.org/abs/1805.08974},
abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classiﬁcation networks on 12 image classiﬁcation datasets. We ﬁnd that, when networks are used as ﬁxed feature extractors or ﬁne-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we ﬁnd that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we ﬁnd that, on two small ﬁne-grained image classiﬁcation datasets, pretraining on ImageNet provides minimal beneﬁts, indicating the learned features from ImageNet do not transfer well to ﬁne-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
language = {en},
urldate = {2020-01-27},
journal = {arXiv:1805.08974 [cs, stat]},
author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
month = jun,
year = {2019},
note = {arXiv: 1805.08974},
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
file = {Kornblith et al. - 2020 - Do Better ImageNet Models Transfer Better.pdf:/home/konstaku/Zotero/storage/XYMF3C3M/Kornblith et al. - 2019 - Do Better ImageNet Models Transfer Better.pdf:application/pdf}
}

@article{classifierPerformance,
title = {Compounding the {Performance} {Improvements} of {Assembled} {Techniques} in a {Convolutional} {Neural} {Network}},
url = {http://arxiv.org/abs/2001.06268},
abstract = {Recent studies in image classiﬁcation have demonstrated a variety of techniques for improving the performance of Convolutional Neural Networks (CNNs). However, attempts to combine existing techniques to create a practical model are still uncommon. In this study, we carry out extensive experiments to validate that carefully assembling these techniques and applying them to a basic CNN model in combination can improve the accuracy and robustness of the model while minimizing the loss of throughput. For example, our proposed ResNet-50 shows an improvement in top-1 accuracy from 76.3\% to 82.78\%, and an mCE improvement from 76.0\% to 48.9\%, on the ImageNet ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. The resulting model signiﬁcantly outperforms state-of-theart models with similar accuracy in terms of mCE and inference throughput. To verify the performance improvement in transfer learning, ﬁne grained classiﬁcation and image retrieval tasks were tested on several open datasets and showed that the improvement to backbone network performance boosted transfer learning performance signiﬁcantly. Our approach achieved 1st place in the iFood Competition Fine-Grained Visual Recognition at CVPR 2019 1, and the source code and trained models will be made publicly available 2.},
language = {en},
urldate = {2020-01-27},
journal = {arXiv:2001.06268 [cs]},
author = {Lee, Jungkyu and Won, Taeryun and Hong, Kiho},
month = jan,
year = {2020},
note = {arXiv: 2001.06268},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Lee et al. - 2020 - Compounding the Performance Improvements of Assemb.pdf:/home/konstaku/Zotero/storage/788PI52C/Lee et al. - 2020 - Compounding the Performance Improvements of Assemb.pdf:application/pdf}
}

@article{leNet,
title = {Gradient-{Based} {Learning} {Applied} to {Document} {Recognition}},
language = {en},
author = {LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Ha, Patrick},
year = {1998},
pages = {46},
file = {LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf:/home/konstaku/Zotero/storage/UISSJ64F/LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf:application/pdf}
}

@article{rethinkTransfer,
title = {Rethinking {ImageNet} {Pre}-{Training}},
abstract = {We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for ﬁne-tuning pretrained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when: (i) using only 10\% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve ﬁnal target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data—a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of ‘pretraining and ﬁne-tuning’ in computer vision.},
language = {en},
author = {He, Kaiming and Girshick, Ross and Dollar, Piotr},
pages = {10},
file = {He et al. - Rethinking ImageNet Pre-Training.pdf:/home/konstaku/Zotero/storage/UBT7GFCE/He et al. - Rethinking ImageNet Pre-Training.pdf:application/pdf}
}

@inproceedings{transferSurvey,
title = {A {Survey} of {Transfer} {Learning} for {Convolutional} {Neural} {Networks}},
doi = {10.1109/SIBGRAPI-T.2019.00010},
abstract = {Transfer learning is an emerging topic that may drive the success of machine learning in research and industry. The lack of data on specific tasks is one of the main reasons to use it, since collecting and labeling data can be very expensive and can take time, and recent concerns with privacy make difficult to use real data from users. The use of transfer learning helps to fast prototype new machine learning models using pre-trained models from a source task since training on millions of images can take time and requires expensive GPUs. In this survey, we review the concepts and definitions related to transfer learning and we list the different terms used in the literature. We bring the point of view from different authors of prior surveys, adding some more recent findings in order to give a clear vision of directions for future work in this field of research.},
booktitle = {2019 32nd {SIBGRAPI} {Conference} on {Graphics}, {Patterns} and {Images} {Tutorials} ({SIBGRAPI}-{T})},
author = {Ribani, Ricardo and Marengoni, Mauricio},
month = oct,
year = {2019},
note = {ISSN: 2474-0691},
keywords = {Machine learning, Convolutional Neural Networks, Data models, Deep Learning, Probability distribution, Supervised learning, Task analysis, Training, Transfer Learning},
pages = {47--57},
file = {IEEE Xplore Abstract Record:/home/konstaku/Zotero/storage/ZCESIJS9/8920338.html:text/html;IEEE Xplore Full Text PDF:/home/konstaku/Zotero/storage/PZN27DVW/Ribani and Marengoni - 2019 - A Survey of Transfer Learning for Convolutional Ne.pdf:application/pdf}
}

@article{transferSurvey2010,
	title = {A {Survey} on {Transfer} {Learning}},
	volume = {22},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5288526/},
	doi = {10.1109/TKDE.2009.191},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classiﬁcation task in one domain of interest, but we only have sufﬁcient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classiﬁcation, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
	language = {en},
	number = {10},
	urldate = {2020-01-29},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	month = oct,
	year = {2010},
	pages = {1345--1359},
	file = {Pan and Yang - 2010 - A Survey on Transfer Learning.pdf:/home/konstaku/Zotero/storage/YZBBSKDB/Pan and Yang - 2010 - A Survey on Transfer Learning.pdf:application/pdf}
}

@article{denseNet,
title = {Densely {Connected} {Convolutional} {Networks}},
url = {http://arxiv.org/abs/1608.06993},
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
language = {en},
urldate = {2020-02-19},
journal = {arXiv:1608.06993 [cs]},
author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
month = jan,
year = {2018},
note = {arXiv: 1608.06993},
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
file = {Huang et al. - 2018 - Densely Connected Convolutional Networks.pdf:/home/konstaku/Zotero/storage/58JA8UJ2/Huang et al. - 2018 - Densely Connected Convolutional Networks.pdf:application/pdf}
}

@article{resNext,
title = {Aggregated {Residual} {Transformations} for {Deep} {Neural} {Networks}},
url = {http://arxiv.org/abs/1611.05431},
abstract = {We present a simple, highly modularized network architecture for image classiﬁcation. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classiﬁcation accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classiﬁcation task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online1.},
language = {en},
urldate = {2020-02-19},
journal = {arXiv:1611.05431 [cs]},
author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
month = apr,
year = {2017},
note = {arXiv: 1611.05431},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Xie et al. - 2017 - Aggregated Residual Transformations for Deep Neura.pdf:/home/konstaku/Zotero/storage/7VQBC36Y/Xie et al. - 2017 - Aggregated Residual Transformations for Deep Neura.pdf:application/pdf}
}
@article{wideResNet,
title = {Wide {Residual} {Networks}},
url = {http://arxiv.org/abs/1605.07146},
abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efﬁciency all previous deep residual networks, including thousand-layerdeep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and signiﬁcant improvements on ImageNet. Our code and models are available at https: //github.com/szagoruyko/wide-residual-networks.},
language = {en},
urldate = {2020-02-19},
journal = {arXiv:1605.07146 [cs]},
author = {Zagoruyko, Sergey and Komodakis, Nikos},
month = jun,
year = {2017},
note = {arXiv: 1605.07146},
keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
file = {Zagoruyko and Komodakis - 2017 - Wide Residual Networks.pdf:/home/konstaku/Zotero/storage/QE833XZI/Zagoruyko and Komodakis - 2017 - Wide Residual Networks.pdf:application/pdf}
}

@article{mobileNetv2,
title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
shorttitle = {{MobileNetV2}},
url = {http://arxiv.org/abs/1801.04381},
abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efﬁcient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.},
language = {en},
urldate = {2020-02-19},
journal = {arXiv:1801.04381 [cs]},
author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
month = mar,
year = {2019},
note = {arXiv: 1801.04381},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
file = {Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:/home/konstaku/Zotero/storage/I4FRFPZ4/Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:application/pdf}
}